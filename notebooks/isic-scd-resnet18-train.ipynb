{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":63056,"databundleVersionId":8940774,"sourceType":"competition"},{"sourceId":187425269,"sourceType":"kernelVersion"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport joblib\nfrom pathlib import Path\nfrom timeit import default_timer as timer\n\nimport pandas as pd\nimport numpy as np\nfrom io import BytesIO\nfrom PIL import Image\nimport h5py\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom timm import create_model\nfrom torch.optim.lr_scheduler import OneCycleLR\n\nfrom accelerate import Accelerator\n\nfrom isic_helper import DotDict\nfrom isic_helper import get_folds\nfrom isic_helper import compute_auc, compute_pauc\nfrom isic_helper import set_seed\nfrom isic_helper import time_to_str","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-10T00:46:34.081479Z","iopub.execute_input":"2024-07-10T00:46:34.082141Z","iopub.status.idle":"2024-07-10T00:46:39.593672Z","shell.execute_reply.started":"2024-07-10T00:46:34.082103Z","shell.execute_reply":"2024-07-10T00:46:39.592564Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"cfg = DotDict()\ncfg.infer = False\ncfg.cpu = False\ncfg.mixed_precision = None\n\ncfg.image_size = 384\ncfg.lr = 5e-4\ncfg.num_epochs = 2\ncfg.seed = 2022\ncfg.train_batch_size = 256\ncfg.train_num_worker = 2\ncfg.val_batch_size = 256\ncfg.val_num_worker = 2\ncfg.log_every = 10\n\ncfg.models_output_dir = \"models\"\ncfg.model_name = \"resnet18_v1\"","metadata":{"execution":{"iopub.status.busy":"2024-07-10T00:46:39.595192Z","iopub.execute_input":"2024-07-10T00:46:39.596265Z","iopub.status.idle":"2024-07-10T00:46:39.603049Z","shell.execute_reply.started":"2024-07-10T00:46:39.596222Z","shell.execute_reply":"2024-07-10T00:46:39.602021Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"INPUT_PATH = Path(\"../input/isic-2024-challenge/\")\nMODELS_OUTPUT_PATH = Path(f\"{cfg.models_output_dir}\")\nMODELS_OUTPUT_PATH.mkdir(exist_ok=True)\n\ntrain_metadata = pd.read_csv(INPUT_PATH / \"train-metadata.csv\", low_memory=False)\ntrain_images = h5py.File(INPUT_PATH / \"train-image.hdf5\", mode=\"r\")\n\nfolds_df = get_folds()\ntrain_metadata = train_metadata.merge(folds_df, on=[\"isic_id\", \"patient_id\"], how=\"inner\")\n# train_metadata = train_metadata.sample(frac=0.05, random_state=cfg.seed)\nprint(f\"Train data size: {train_metadata.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-10T00:46:39.604433Z","iopub.execute_input":"2024-07-10T00:46:39.604795Z","iopub.status.idle":"2024-07-10T00:46:48.417051Z","shell.execute_reply.started":"2024-07-10T00:46:39.604768Z","shell.execute_reply":"2024-07-10T00:46:48.415973Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Train data size: (401059, 57)\n","output_type":"stream"}]},{"cell_type":"code","source":"id_column = \"isic_id\"\ntarget_column = \"final_target\"\ngroup_column = \"patient_id\"\n\ntrain_ids = train_metadata[id_column]\ngroups = train_metadata[group_column]\nfolds = train_metadata[\"fold\"]\ny_train = train_metadata[target_column]","metadata":{"execution":{"iopub.status.busy":"2024-07-10T00:46:48.419778Z","iopub.execute_input":"2024-07-10T00:46:48.420151Z","iopub.status.idle":"2024-07-10T00:46:48.426190Z","shell.execute_reply.started":"2024-07-10T00:46:48.420121Z","shell.execute_reply":"2024-07-10T00:46:48.425061Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"accelerator = Accelerator(cpu=cfg.cpu, mixed_precision=cfg.mixed_precision)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T00:46:48.427783Z","iopub.execute_input":"2024-07-10T00:46:48.428210Z","iopub.status.idle":"2024-07-10T00:46:48.501348Z","shell.execute_reply.started":"2024-07-10T00:46:48.428173Z","shell.execute_reply":"2024-07-10T00:46:48.500241Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def dev_augment(image_size):\n    transform = A.Compose([\n        A.Resize(image_size, image_size),\n#         A.Normalize(\n#             mean=[0., 0., 0.],\n#             std=[1, 1, 1],\n#             max_pixel_value=255.0,\n#             p=1.0\n#         ),\n        ToTensorV2()\n    ], p=1.)\n    return transform\n\ndef val_augment(image_size):\n    transform = A.Compose([\n        A.Resize(image_size, image_size),\n#         A.Normalize(\n#             mean=[0., 0., 0.],\n#             std=[1, 1, 1],\n#             max_pixel_value=255.0,\n#             p=1.0\n#         ),\n        ToTensorV2()\n    ], p=1.)\n    return transform\n\nclass ISICDataset(Dataset):\n    def __init__(self, metadata, images, augment, infer=False):\n        self.metadata = metadata\n        self.images = images\n        self.augment = augment\n        self.length = len(self.metadata)\n        self.infer = infer\n    \n    def __len__(self):\n        return self.length\n    \n    def __getitem__(self, index):\n        data = self.metadata.iloc[index]\n        \n        image = np.array(Image.open(BytesIO(self.images[data[id_column]][()])))\n        image = self.augment(image=image)[\"image\"]\n        \n        record = {\n            \"index\": index,\n            \"image\": image\n        }\n        \n        if not self.infer:\n            target = data[target_column]\n            record[\"target\"] = target\n        \n        return record\n\nclass ISICNet(nn.Module):\n    def __init__(self, arch=\"resnet18\", pretrained=False, infer=False):\n        super(ISICNet, self).__init__()\n        self.infer = infer\n        self.model = create_model(model_name=arch, pretrained=pretrained, in_chans=3,  num_classes=0, global_pool='')\n        self.classifier = nn.Linear(self.model.num_features, 1)\n        \n        self.dropout = nn.ModuleList([nn.Dropout(0.5) for i in range(5)])\n        \n    def forward(self, batch):\n        image = batch[\"image\"]\n        image = image.float() / 255\n        \n        x = self.model(image)\n        bs = len(image)\n        pool = F.adaptive_avg_pool2d(x, 1).reshape(bs,-1)\n        \n        if self.training:\n            logit = 0\n            for i in range(len(self.dropout)):\n                logit += self.classifier(self.dropout[i](pool))\n            logit = logit/len(self.dropout)\n        else:\n            logit = self.classifier(pool)\n            \n        output = {}\n        output[\"preds\"] = torch.sigmoid(logit)\n        \n        if not self.infer:\n            target = batch[\"target\"].unsqueeze(1)\n            output[\"bce_loss\"] = F.binary_cross_entropy_with_logits(logit.float(), target.float())\n            \n        return output","metadata":{"execution":{"iopub.status.busy":"2024-07-10T00:46:48.502647Z","iopub.execute_input":"2024-07-10T00:46:48.502973Z","iopub.status.idle":"2024-07-10T00:46:48.522026Z","shell.execute_reply.started":"2024-07-10T00:46:48.502947Z","shell.execute_reply":"2024-07-10T00:46:48.520894Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"best_num_epochs = {}\nval_auc_scores = {}\nval_pauc_scores = {}\nall_folds = np.sort(folds.unique())\noof_predictions = np.zeros(train_metadata.shape[0])\nfor fold in all_folds:\n    set_seed(cfg.seed)\n    \n    print(f\"Running fold: {fold}\")\n    dev_index = folds != fold\n    val_index = folds == fold\n    \n    dev_metadata = train_metadata[train_metadata[\"fold\"] != fold].reset_index(drop=True);print(f\"Dev data size: {dev_metadata.shape}\")\n    val_metadata = train_metadata[train_metadata[\"fold\"] == fold].reset_index(drop=True);print(f\"Val data size: {val_metadata.shape}\")\n    \n    dev_dataset = ISICDataset(dev_metadata, train_images, augment=dev_augment(image_size=cfg.image_size))\n    val_dataset = ISICDataset(val_metadata, train_images, augment=val_augment(image_size=cfg.image_size))\n\n    dev_dataloader = DataLoader(dev_dataset, shuffle=True, batch_size=cfg.train_batch_size, num_workers=cfg.train_num_worker)\n    val_dataloader = DataLoader(val_dataset, shuffle=False, batch_size=cfg.val_batch_size, num_workers=cfg.val_num_worker, drop_last=False)\n    \n    net = ISICNet(pretrained=True)\n    net = net.to(accelerator.device)\n    \n    for param in net.parameters():\n        param.requires_grad = False\n\n    for param in net.classifier.parameters():\n        param.requires_grad = True\n        \n    optimizer = torch.optim.Adam(params=net.parameters(), lr=cfg.lr)\n    lr_scheduler = OneCycleLR(optimizer=optimizer, max_lr=cfg.lr, epochs=cfg.num_epochs, steps_per_epoch=len(dev_dataloader))\n\n    net, optimizer, dev_dataloader, val_dataloader, lr_scheduler = accelerator.prepare(\n        net, optimizer, dev_dataloader, val_dataloader, lr_scheduler\n    )\n    \n    print(\"Ready to train\")\n    \n    overall_step = 0\n    starting_epoch = 0\n    best_pauc_score = -np.Inf\n    best_auc_score = -np.Inf\n    best_epoch = None\n    best_val_preds = None\n\n    for epoch in range(starting_epoch, cfg.num_epochs):\n        net.train()\n        count = 0\n        start_timer = timer()\n        for step, batch in enumerate(dev_dataloader):\n            # We could avoid this line since we set the accelerator with `device_placement=True`.\n            batch = {k: v.to(accelerator.device) for k, v in batch.items()}\n            output = net(batch)\n            loss = output[\"bce_loss\"]\n            accelerator.backward(loss)\n            optimizer.step()\n#             lr_scheduler.step()\n            optimizer.zero_grad()\n            overall_step += 1\n            count += len(batch[\"index\"])\n            time_lapsed = time_to_str(timer() - start_timer, \"min\")\n            if step % cfg.log_every == 0:\n                print(f'\\rTraining: {count}/{len(dev_dataset)}',\n                          time_lapsed,\n                          end='', flush=True)\n        print(f\"\\nTraining done: {time_lapsed}\")\n\n        net.eval()\n        val_preds = []\n        val_y = []\n        count = 0\n        start_timer = timer()\n        for step, batch in enumerate(val_dataloader):\n            # We could avoid this line since we set the accelerator with `device_placement=True`.\n            batch = {k: v.to(accelerator.device) for k, v in batch.items()}\n            with torch.no_grad():\n                output = net(batch)\n            val_preds_batch = output[\"preds\"]\n            val_y_batch = batch[\"target\"]\n            val_preds_batch, val_y_batch = accelerator.gather_for_metrics((val_preds_batch, val_y_batch))\n            val_preds.append(val_preds_batch.data.cpu().numpy().reshape(-1))\n            val_y.append(val_y_batch.data.cpu().numpy().reshape(-1))\n            count += len(batch[\"index\"])\n            time_lapsed = time_to_str(timer() - start_timer, \"min\")\n            if step % cfg.log_every == 0:\n                print(f'\\rValidation: {count}/{len(val_dataset)}',\n                          time_lapsed,\n                          end='', flush=True)\n        print(f\"Validation done: {time_lapsed}\")\n        print('')\n        val_preds = np.concatenate(val_preds)\n        val_y = np.concatenate(val_y)\n        auc = compute_auc(val_y, val_preds) \n        pauc = compute_pauc(val_y, val_preds, min_tpr=0.80)\n        \n        if pauc >= best_pauc_score:\n            best_auc_score = auc\n            best_pauc_score = pauc\n            best_epoch = epoch\n            best_val_preds = val_preds\n        print(f\"Epoch pauc: {pauc} | Best auc: {best_auc_score} | Best pauc: {best_pauc_score} | Best epoch: {best_epoch}\")\n        \n        output_dir = f\"fold_{fold}/model_{cfg.model_name}_epoch_{epoch}\"\n        if cfg.models_output_dir is not None:\n            output_dir = Path(f\"{cfg.models_output_dir}/{output_dir}\")\n        accelerator.save_state(output_dir)\n    \n    best_num_epochs[f\"fold_{fold}\"] = best_epoch\n    val_auc_scores[f\"fold_{fold}\"] = best_auc_score\n    val_pauc_scores[f\"fold_{fold}\"] = best_pauc_score\n    \n    oof_predictions[val_index] = best_val_preds\n    print(\"\\n\")\n    break","metadata":{"execution":{"iopub.status.busy":"2024-07-10T00:46:48.523956Z","iopub.execute_input":"2024-07-10T00:46:48.524338Z","iopub.status.idle":"2024-07-10T00:59:33.214268Z","shell.execute_reply.started":"2024-07-10T00:46:48.524307Z","shell.execute_reply":"2024-07-10T00:59:33.212724Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Running fold: 1\nDev data size: (320845, 57)\nVal data size: (80214, 57)\nReady to train\nTraining: 320256/320845 10 min 19.31 sec\nTraining done: 10 min 20.44 sec\nValidation: 79616/80214 2 min 13.05 secValidation done: 2 min 13.92 sec\n\nEpoch pauc: 0.03325196834187686 | Best auc: 0.5527043055650315 | Best pauc: 0.03325196834187686 | Best epoch: 0\nTraining: 2816/320845 0 min 6.07 sec","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     50\u001b[0m start_timer \u001b[38;5;241m=\u001b[39m timer()\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dev_dataloader):\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# We could avoid this line since we set the accelerator with `device_placement=True`.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(accelerator\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     54\u001b[0m     output \u001b[38;5;241m=\u001b[39m net(batch)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/data_loader.py:463\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 463\u001b[0m         current_batch \u001b[38;5;241m=\u001b[39m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_non_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m     next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:186\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m         skip_keys \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[0;32m--> 186\u001b[0m         {\n\u001b[1;32m    187\u001b[0m             k: t \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m skip_keys \u001b[38;5;28;01melse\u001b[39;00m send_to_device(t, device, non_blocking\u001b[38;5;241m=\u001b[39mnon_blocking, skip_keys\u001b[38;5;241m=\u001b[39mskip_keys)\n\u001b[1;32m    188\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    189\u001b[0m         }\n\u001b[1;32m    190\u001b[0m     )\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:187\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m         skip_keys \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[1;32m    186\u001b[0m         {\n\u001b[0;32m--> 187\u001b[0m             k: t \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m skip_keys \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    189\u001b[0m         }\n\u001b[1;32m    190\u001b[0m     )\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:158\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    156\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mto(device)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"Running fold: 1\nDev data size: (16040, 57)\nVal data size: (4013, 57)\nReady to train\nTraining: 15616/16040 0 min 26.57 sec\nTraining done: 0 min 27.31 sec\nValidation: 2816/4013 0 min 6.35 secValidation done: 0 min 8.14 sec\n\nEpoch pauc: 0.11899302093718842 | Best auc: 0.5949651046859421 | Best pauc: 0.11899302093718842 | Best epoch: 0\nTraining: 15616/16040 0 min 26.21 sec\nTraining done: 0 min 26.96 sec\nValidation: 2816/4013 0 min 5.90 secValidation done: 0 min 7.84 sec\n\nEpoch pauc: 0.17547357926221333 | Best auc: 0.8773678963110668 | Best pauc: 0.17547357926221333 | Best epoch: 1","metadata":{"execution":{"iopub.status.busy":"2024-07-10T00:45:50.421622Z","iopub.execute_input":"2024-07-10T00:45:50.422608Z","iopub.status.idle":"2024-07-10T00:45:50.432755Z","shell.execute_reply.started":"2024-07-10T00:45:50.422567Z","shell.execute_reply":"2024-07-10T00:45:50.431144Z"},"trusted":true},"execution_count":10,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[10], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    Running fold: 1\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (474539499.py, line 1)","output_type":"error"}]},{"cell_type":"code","source":"oof_preds_df = pd.DataFrame({\n    id_column: train_ids,\n    group_column: groups,\n    \"fold\": folds,\n    target_column: y_train,\n    f\"oof_{cfg.model_name}\": oof_predictions\n})\noof_preds_df.to_csv(f\"oof_preds_{cfg.model_name}.csv\")\noof_preds_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_num_epochs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_auc_scores","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_pauc_scores","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_auc_oof = compute_auc(oof_preds_df[target_column], oof_preds_df[f\"oof_{cfg.model_name}\"])\ncv_pauc_oof = compute_pauc(oof_preds_df[target_column], oof_preds_df[f\"oof_{cfg.model_name}\"], min_tpr=0.8)\n\ncv_auc_avg = np.mean(list(val_auc_scores.values()))\ncv_pauc_avg = np.mean(list(val_pauc_scores.values()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"CV AUC OOF: {cv_auc_oof}\")\nprint(f\"CV PAUC OOF: {cv_pauc_oof}\")\nprint(f\"CV AUC AVG: {cv_auc_avg}\")\nprint(f\"CV PAUC AVG: {cv_pauc_avg}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = vars(cfg)\nparams = {k: v for k, v in params.items() if not k.startswith(\"_\")}\n\nmetadata = {\n    \"params\": params,\n    \"best_num_epochs\": best_num_epochs,\n    \"val_auc_scores\": val_auc_scores,\n    \"val_pauc_scores\": val_pauc_scores,\n    \"cv_auc_oof\": cv_auc_oof,\n    \"cv_pauc_oof\": cv_pauc_oof,\n    \"cv_auc_avg\": cv_auc_avg,\n    \"cv_pauc_avg\": cv_pauc_avg\n}\n\nwith open(\"run_metadata.json\", \"w\") as f:\n    json.dump(metadata, f)","metadata":{},"execution_count":null,"outputs":[]}]}