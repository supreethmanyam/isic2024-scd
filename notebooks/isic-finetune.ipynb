{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c85c0fa1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-13T19:55:49.902931Z",
     "iopub.status.busy": "2024-08-13T19:55:49.902088Z",
     "iopub.status.idle": "2024-08-13T19:55:58.975875Z",
     "shell.execute_reply": "2024-08-13T19:55:58.975063Z"
    },
    "papermill": {
     "duration": 9.08179,
     "end_time": "2024-08-13T19:55:58.978182",
     "exception": false,
     "start_time": "2024-08-13T19:55:49.896392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import (\n",
    "    DistributedDataParallelKwargs,\n",
    "    ProjectConfiguration,\n",
    "    set_seed,\n",
    ")\n",
    "from accelerate.logging import get_logger\n",
    "from io import BytesIO\n",
    "\n",
    "import albumentations as A\n",
    "import h5py\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from timm import create_model\n",
    "import torch.nn.functional as F\n",
    "from safetensors import safe_open\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from isic_helper import get_folds\n",
    "from isic_helper import compute_auc, compute_pauc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7af1ee0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-13T19:55:58.986765Z",
     "iopub.status.busy": "2024-08-13T19:55:58.986453Z",
     "iopub.status.idle": "2024-08-13T19:55:58.994307Z",
     "shell.execute_reply": "2024-08-13T19:55:58.993459Z"
    },
    "papermill": {
     "duration": 0.013988,
     "end_time": "2024-08-13T19:55:58.996188",
     "exception": false,
     "start_time": "2024-08-13T19:55:58.982200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    mixed_precision: bool = \"fp16\"\n",
    "    image_size: int = 64\n",
    "    train_batch_size: int = 64\n",
    "    val_batch_size: int = 512\n",
    "    num_workers: int = 4\n",
    "    init_lr: float = 3e-5\n",
    "    num_epochs: int = 20\n",
    "    n_tta: int = 8\n",
    "    seed: int = 2022\n",
    "\n",
    "    ext: str = \"2020,2019\"\n",
    "    only_malignant: bool = False\n",
    "    debug: bool = False\n",
    "\n",
    "args = Config()\n",
    "args.model_name = \"efficientnet_b0\"\n",
    "args.version = \"v3\"\n",
    "args.model_identifier = f\"{args.model_name}_{args.version}\"\n",
    "args.pretrained_weights_path = f\"/kaggle/input/isic-scd-{args.model_name.replace('_', '-')}-{args.version}-train\"\n",
    "args.fold = 1\n",
    "args.model_dir = f\"{args.model_identifier}_finetune\"\n",
    "args.logging_dir = \"logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dd0b3fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-13T19:55:59.004345Z",
     "iopub.status.busy": "2024-08-13T19:55:59.004099Z",
     "iopub.status.idle": "2024-08-13T19:55:59.038183Z",
     "shell.execute_reply": "2024-08-13T19:55:59.037461Z"
    },
    "papermill": {
     "duration": 0.040451,
     "end_time": "2024-08-13T19:55:59.040048",
     "exception": false,
     "start_time": "2024-08-13T19:55:58.999597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dev_augment(image_size, mean=None, std=None):\n",
    "    if mean is not None and std is not None:\n",
    "        normalize = A.Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0)\n",
    "    else:\n",
    "        normalize = A.Normalize(max_pixel_value=255.0, p=1.0)\n",
    "    transform = A.Compose(\n",
    "        [\n",
    "            A.Transpose(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.RandomBrightnessContrast(\n",
    "                brightness_limit=0.2, contrast_limit=0.2, p=0.75\n",
    "            ),\n",
    "            A.OneOf(\n",
    "                [\n",
    "                    A.MotionBlur(blur_limit=(5, 7)),\n",
    "                    A.MedianBlur(blur_limit=(5, 7)),\n",
    "                    A.GaussianBlur(blur_limit=(5, 7)),\n",
    "                    A.GaussNoise(var_limit=(5.0, 30.0)),\n",
    "                ],\n",
    "                p=0.7,\n",
    "            ),\n",
    "            A.OneOf(\n",
    "                [\n",
    "                    A.OpticalDistortion(distort_limit=1.0),\n",
    "                    A.GridDistortion(num_steps=5, distort_limit=1.0),\n",
    "                    A.ElasticTransform(alpha=3),\n",
    "                ],\n",
    "                p=0.7,\n",
    "            ),\n",
    "            A.CLAHE(clip_limit=4.0, p=0.7),\n",
    "            A.HueSaturationValue(\n",
    "                hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5\n",
    "            ),\n",
    "            A.ShiftScaleRotate(\n",
    "                shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.85\n",
    "            ),\n",
    "            A.Resize(image_size, image_size),\n",
    "            A.CoarseDropout(\n",
    "                max_height=int(image_size * 0.375),\n",
    "                max_width=int(image_size * 0.375),\n",
    "                max_holes=1,\n",
    "                min_holes=1,\n",
    "                p=0.7,\n",
    "            ),\n",
    "            normalize,\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "        p=1.0,\n",
    "    )\n",
    "    return transform\n",
    "\n",
    "\n",
    "def val_augment(image_size, mean=None, std=None):\n",
    "    if mean is not None and std is not None:\n",
    "        normalize = A.Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0)\n",
    "    else:\n",
    "        normalize = A.Normalize(max_pixel_value=255.0, p=1.0)\n",
    "    transform = A.Compose(\n",
    "        [A.Resize(image_size, image_size), normalize, ToTensorV2()], p=1.0\n",
    "    )\n",
    "    return transform\n",
    "\n",
    "\n",
    "class ISICDataset(Dataset):\n",
    "    def __init__(self, metadata, images, augment, infer=False):\n",
    "        self.metadata = metadata\n",
    "        self.images = images\n",
    "        self.augment = augment\n",
    "        self.length = len(self.metadata)\n",
    "        self.infer = infer\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.metadata.iloc[index]\n",
    "        image = np.array(Image.open(BytesIO(self.images[row[\"isic_id\"]][()])))\n",
    "        if self.augment is not None:\n",
    "            image = self.augment(image=image)[\"image\"].float()\n",
    "        if self.infer:\n",
    "            return image\n",
    "        else:\n",
    "            target = torch.tensor(row[\"target\"])\n",
    "            return image, target\n",
    "\n",
    "class ISICNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name\n",
    "    ):\n",
    "        super(ISICNet, self).__init__()\n",
    "        self.model = create_model(\n",
    "            model_name=model_name,\n",
    "            pretrained=False,\n",
    "            in_chans=3,\n",
    "            num_classes=0,\n",
    "            global_pool=\"\",\n",
    "        )\n",
    "        in_dim = self.model.num_features\n",
    "        self.classifier = nn.Linear(in_dim, 1)\n",
    "        self.dropouts = nn.ModuleList([nn.Dropout(0.5) for _ in range(5)])\n",
    "\n",
    "    def forward(self, images):\n",
    "        x = self.model(images)\n",
    "        bs = len(images)\n",
    "        pool = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)\n",
    "        if self.training:\n",
    "            logits = 0\n",
    "            for i in range(len(self.dropouts)):\n",
    "                logits += self.classifier(self.dropouts[i](pool))\n",
    "            logits = logits / len(self.dropouts)\n",
    "        else:\n",
    "            logits = self.classifier(pool)\n",
    "        return logits\n",
    "\n",
    "def train_epoch(\n",
    "    epoch,\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    dev_dataloader,\n",
    "    lr_scheduler,\n",
    "    accelerator,\n",
    "    log_interval=100,\n",
    "):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    total_steps = len(dev_dataloader)\n",
    "    for step, (images, targets) in enumerate(dev_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        targets = targets.float().unsqueeze(1)\n",
    "        loss = criterion(probs, targets)\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        loss_value = accelerator.gather(loss).item()\n",
    "        train_loss.append(loss_value)\n",
    "        smooth_loss = sum(train_loss[-100:]) / min(len(train_loss), 100)\n",
    "        if (step == 0) or ((step + 1) % log_interval == 0):\n",
    "            print(\n",
    "                f\"Epoch: {epoch} | Step: {step + 1}/{total_steps} |\"\n",
    "                f\" Loss: {loss_value:.5f} | Smooth loss: {smooth_loss:.5f}\"\n",
    "            )\n",
    "    train_loss = np.mean(train_loss)\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def get_trans(img, iteration):\n",
    "    if iteration >= 6:\n",
    "        img = img.transpose(2, 3)\n",
    "    if iteration % 6 == 0:\n",
    "        return img\n",
    "    elif iteration % 6 == 1:\n",
    "        return torch.flip(img, dims=[2])\n",
    "    elif iteration % 6 == 2:\n",
    "        return torch.flip(img, dims=[3])\n",
    "    elif iteration % 6 == 3:\n",
    "        return torch.rot90(img, 1, dims=[2, 3])\n",
    "    elif iteration % 6 == 4:\n",
    "        return torch.rot90(img, 2, dims=[2, 3])\n",
    "    elif iteration % 6 == 5:\n",
    "        return torch.rot90(img, 3, dims=[2, 3])\n",
    "\n",
    "\n",
    "def val_epoch(\n",
    "    epoch,\n",
    "    model,\n",
    "    criterion,\n",
    "    val_dataloader,\n",
    "    accelerator,\n",
    "    n_tta,\n",
    "    log_interval=10,\n",
    "):\n",
    "    model.eval()\n",
    "    val_probs = []\n",
    "    val_targets = []\n",
    "    val_loss = []\n",
    "    total_steps = len(val_dataloader)\n",
    "    with torch.no_grad():\n",
    "        for step, (images, targets) in enumerate(val_dataloader):\n",
    "            logits = 0\n",
    "            probs = 0\n",
    "            for i in range(n_tta):\n",
    "                logits_iter = model(get_trans(images, i))\n",
    "                logits += logits_iter\n",
    "                probs += torch.sigmoid(logits_iter)\n",
    "            logits /= n_tta\n",
    "            probs /= n_tta\n",
    "\n",
    "            targets = targets.float().unsqueeze(1)\n",
    "            loss = criterion(probs, targets)\n",
    "            val_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "            probs, targets = accelerator.gather((probs, targets))\n",
    "            val_probs.append(probs)\n",
    "            val_targets.append(targets)\n",
    "\n",
    "            if (step == 0) or ((step + 1) % log_interval == 0):\n",
    "                print(f\"Epoch: {epoch} | Step: {step + 1}/{total_steps}\")\n",
    "\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_probs = torch.cat(val_probs).cpu().numpy()\n",
    "    val_targets = torch.cat(val_targets).cpu().numpy()\n",
    "    val_auc = compute_auc(val_targets, val_probs)\n",
    "    val_pauc = compute_pauc(val_targets, val_probs, min_tpr=0.8)\n",
    "    return (\n",
    "        val_loss,\n",
    "        val_auc,\n",
    "        val_pauc,\n",
    "        val_probs,\n",
    "        val_targets,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fef14a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-13T19:55:59.048297Z",
     "iopub.status.busy": "2024-08-13T19:55:59.047588Z",
     "iopub.status.idle": "2024-08-13T19:55:59.139129Z",
     "shell.execute_reply": "2024-08-13T19:55:59.138136Z"
    },
    "papermill": {
     "duration": 0.097927,
     "end_time": "2024-08-13T19:55:59.141393",
     "exception": false,
     "start_time": "2024-08-13T19:55:59.043466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger = get_logger(__name__)\n",
    "logging_dir = Path(args.model_dir, args.logging_dir)\n",
    "accelerator_project_config = ProjectConfiguration(\n",
    "    project_dir=args.model_dir, logging_dir=str(logging_dir)\n",
    ")\n",
    "kwargs = DistributedDataParallelKwargs()\n",
    "accelerator = Accelerator(\n",
    "    mixed_precision=args.mixed_precision,\n",
    "    project_config=accelerator_project_config,\n",
    "    kwargs_handlers=[kwargs],\n",
    ")\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "print(accelerator.state)\n",
    "\n",
    "if args.seed is not None:\n",
    "    set_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1798b9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-13T19:55:59.150553Z",
     "iopub.status.busy": "2024-08-13T19:55:59.150233Z",
     "iopub.status.idle": "2024-08-13T19:56:09.399184Z",
     "shell.execute_reply": "2024-08-13T19:56:09.398276Z"
    },
    "papermill": {
     "duration": 10.255534,
     "end_time": "2024-08-13T19:56:09.401407",
     "exception": false,
     "start_time": "2024-08-13T19:55:59.145873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: (401059, 57)\n",
      "Test data size: (3, 44)\n"
     ]
    }
   ],
   "source": [
    "id_column = \"isic_id\"\n",
    "target_column = \"target\"\n",
    "group_column = \"patient_id\"\n",
    "\n",
    "INPUT_PATH = Path(\"/kaggle/input/isic-2024-challenge/\")\n",
    "\n",
    "train_metadata = pd.read_csv(INPUT_PATH / \"train-metadata.csv\", low_memory=False, na_values=[\"NA\"])\n",
    "test_metadata = pd.read_csv(INPUT_PATH / \"test-metadata.csv\", low_memory=False, na_values=[\"NA\"])\n",
    "\n",
    "folds_df = get_folds()\n",
    "train_metadata = train_metadata.merge(folds_df, on=[\"isic_id\", \"patient_id\"], how=\"inner\")\n",
    "print(f\"Train data size: {train_metadata.shape}\")\n",
    "print(f\"Test data size: {test_metadata.shape}\")\n",
    "\n",
    "train_images = h5py.File(INPUT_PATH / \"train-image.hdf5\", mode=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0657893b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-13T19:56:09.410317Z",
     "iopub.status.busy": "2024-08-13T19:56:09.410022Z",
     "iopub.status.idle": "2024-08-13T19:56:10.162777Z",
     "shell.execute_reply": "2024-08-13T19:56:10.161974Z"
    },
    "papermill": {
     "duration": 0.759776,
     "end_time": "2024-08-13T19:56:10.165060",
     "exception": false,
     "start_time": "2024-08-13T19:56:09.405284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if args.debug:\n",
    "    args.num_epochs = 2\n",
    "    dev_index = (\n",
    "        train_metadata[train_metadata[\"fold\"] != args.fold]\n",
    "        .sample(args.train_batch_size * 3, random_state=args.seed)\n",
    "        .index\n",
    "    )\n",
    "    val_index = (\n",
    "        train_metadata[train_metadata[\"fold\"] == args.fold]\n",
    "        .sample(args.val_batch_size * 10, random_state=args.seed)\n",
    "        .index\n",
    "    )\n",
    "else:\n",
    "    dev_index = train_metadata[train_metadata[\"fold\"] != args.fold].index\n",
    "    val_index = train_metadata[train_metadata[\"fold\"] == args.fold].index\n",
    "\n",
    "dev_metadata = train_metadata.loc[dev_index, :].reset_index(drop=True)\n",
    "val_metadata = train_metadata.loc[val_index, :].reset_index(drop=True)\n",
    "\n",
    "mean = None\n",
    "std = None\n",
    "\n",
    "dev_dataset = ISICDataset(\n",
    "    dev_metadata,\n",
    "    train_images,\n",
    "    augment=dev_augment(args.image_size, mean=mean, std=std),\n",
    "    infer=False,\n",
    ")\n",
    "val_dataset = ISICDataset(\n",
    "    val_metadata,\n",
    "    train_images,\n",
    "    augment=val_augment(args.image_size, mean=mean, std=std),\n",
    "    infer=False,\n",
    ")\n",
    "\n",
    "sampler = RandomSampler(dev_dataset)\n",
    "\n",
    "dev_dataloader = DataLoader(\n",
    "    dev_dataset,\n",
    "    batch_size=args.train_batch_size,\n",
    "    sampler=sampler,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=args.val_batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=args.num_workers,\n",
    "    drop_last=False,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a8a50a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-13T19:56:10.173882Z",
     "iopub.status.busy": "2024-08-13T19:56:10.173580Z",
     "iopub.status.idle": "2024-08-13T21:16:52.577267Z",
     "shell.execute_reply": "2024-08-13T21:16:52.576290Z"
    },
    "papermill": {
     "duration": 4842.410487,
     "end_time": "2024-08-13T21:16:52.579412",
     "exception": false,
     "start_time": "2024-08-13T19:56:10.168925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=['classifier.weight', 'classifier.bias'], unexpected_keys=[])\n",
      "Fold 1 | Epoch 1\n",
      "Epoch: 1 | Step: 1/5014 | Loss: 0.58040 | Smooth loss: 0.58040\n",
      "Epoch: 1 | Step: 100/5014 | Loss: 0.20837 | Smooth loss: 0.35927\n",
      "Epoch: 1 | Step: 200/5014 | Loss: 0.07850 | Smooth loss: 0.12965\n",
      "Epoch: 1 | Step: 300/5014 | Loss: 0.03784 | Smooth loss: 0.05594\n",
      "Epoch: 1 | Step: 400/5014 | Loss: 0.02317 | Smooth loss: 0.03373\n",
      "Epoch: 1 | Step: 500/5014 | Loss: 0.01504 | Smooth loss: 0.01932\n",
      "Epoch: 1 | Step: 600/5014 | Loss: 0.01120 | Smooth loss: 0.02051\n",
      "Epoch: 1 | Step: 700/5014 | Loss: 0.00844 | Smooth loss: 0.01341\n",
      "Epoch: 1 | Step: 800/5014 | Loss: 0.00664 | Smooth loss: 0.01156\n",
      "Epoch: 1 | Step: 900/5014 | Loss: 0.00514 | Smooth loss: 0.01121\n",
      "Epoch: 1 | Step: 1000/5014 | Loss: 0.00434 | Smooth loss: 0.01023\n",
      "Epoch: 1 | Step: 1100/5014 | Loss: 0.00377 | Smooth loss: 0.01024\n",
      "Epoch: 1 | Step: 1200/5014 | Loss: 0.00358 | Smooth loss: 0.00869\n",
      "Epoch: 1 | Step: 1300/5014 | Loss: 0.00286 | Smooth loss: 0.01024\n",
      "Epoch: 1 | Step: 1400/5014 | Loss: 0.00256 | Smooth loss: 0.00425\n",
      "Epoch: 1 | Step: 1500/5014 | Loss: 0.00219 | Smooth loss: 0.00730\n",
      "Epoch: 1 | Step: 1600/5014 | Loss: 0.00162 | Smooth loss: 0.00632\n",
      "Epoch: 1 | Step: 1700/5014 | Loss: 0.00194 | Smooth loss: 0.00616\n",
      "Epoch: 1 | Step: 1800/5014 | Loss: 0.00150 | Smooth loss: 0.00777\n",
      "Epoch: 1 | Step: 1900/5014 | Loss: 0.00132 | Smooth loss: 0.00546\n",
      "Epoch: 1 | Step: 2000/5014 | Loss: 0.00191 | Smooth loss: 0.00733\n",
      "Epoch: 1 | Step: 2100/5014 | Loss: 0.00098 | Smooth loss: 0.00343\n",
      "Epoch: 1 | Step: 2200/5014 | Loss: 0.07831 | Smooth loss: 0.00697\n",
      "Epoch: 1 | Step: 2300/5014 | Loss: 0.00171 | Smooth loss: 0.00865\n",
      "Epoch: 1 | Step: 2400/5014 | Loss: 0.00171 | Smooth loss: 0.00904\n",
      "Epoch: 1 | Step: 2500/5014 | Loss: 0.00135 | Smooth loss: 0.00631\n",
      "Epoch: 1 | Step: 2600/5014 | Loss: 0.00165 | Smooth loss: 0.00918\n",
      "Epoch: 1 | Step: 2700/5014 | Loss: 0.11617 | Smooth loss: 0.00205\n",
      "Epoch: 1 | Step: 2800/5014 | Loss: 0.00212 | Smooth loss: 0.00748\n",
      "Epoch: 1 | Step: 2900/5014 | Loss: 0.00124 | Smooth loss: 0.00672\n",
      "Epoch: 1 | Step: 3000/5014 | Loss: 0.00222 | Smooth loss: 0.00731\n",
      "Epoch: 1 | Step: 3100/5014 | Loss: 0.00129 | Smooth loss: 0.01042\n",
      "Epoch: 1 | Step: 3200/5014 | Loss: 0.00094 | Smooth loss: 0.00612\n",
      "Epoch: 1 | Step: 3300/5014 | Loss: 0.00067 | Smooth loss: 0.00423\n",
      "Epoch: 1 | Step: 3400/5014 | Loss: 0.00069 | Smooth loss: 0.00477\n",
      "Epoch: 1 | Step: 3500/5014 | Loss: 0.00053 | Smooth loss: 0.00558\n",
      "Epoch: 1 | Step: 3600/5014 | Loss: 0.00098 | Smooth loss: 0.00752\n",
      "Epoch: 1 | Step: 3700/5014 | Loss: 0.00115 | Smooth loss: 0.00844\n",
      "Epoch: 1 | Step: 3800/5014 | Loss: 0.00081 | Smooth loss: 0.00702\n",
      "Epoch: 1 | Step: 3900/5014 | Loss: 0.00089 | Smooth loss: 0.00687\n",
      "Epoch: 1 | Step: 4000/5014 | Loss: 0.00139 | Smooth loss: 0.00767\n",
      "Epoch: 1 | Step: 4100/5014 | Loss: 0.00113 | Smooth loss: 0.00826\n",
      "Epoch: 1 | Step: 4200/5014 | Loss: 0.00123 | Smooth loss: 0.01072\n",
      "Epoch: 1 | Step: 4300/5014 | Loss: 0.00165 | Smooth loss: 0.01029\n",
      "Epoch: 1 | Step: 4400/5014 | Loss: 0.00066 | Smooth loss: 0.00266\n",
      "Epoch: 1 | Step: 4500/5014 | Loss: 0.00130 | Smooth loss: 0.00804\n",
      "Epoch: 1 | Step: 4600/5014 | Loss: 0.00066 | Smooth loss: 0.00264\n",
      "Epoch: 1 | Step: 4700/5014 | Loss: 0.00073 | Smooth loss: 0.00641\n",
      "Epoch: 1 | Step: 4800/5014 | Loss: 0.00170 | Smooth loss: 0.00771\n",
      "Epoch: 1 | Step: 4900/5014 | Loss: 0.00115 | Smooth loss: 0.00668\n",
      "Epoch: 1 | Step: 5000/5014 | Loss: 0.00075 | Smooth loss: 0.00641\n",
      "Epoch: 1 | Step: 1/157\n",
      "Epoch: 1 | Step: 10/157\n",
      "Epoch: 1 | Step: 20/157\n",
      "Epoch: 1 | Step: 30/157\n",
      "Epoch: 1 | Step: 40/157\n",
      "Epoch: 1 | Step: 50/157\n",
      "Epoch: 1 | Step: 60/157\n",
      "Epoch: 1 | Step: 70/157\n",
      "Epoch: 1 | Step: 80/157\n",
      "Epoch: 1 | Step: 90/157\n",
      "Epoch: 1 | Step: 100/157\n",
      "Epoch: 1 | Step: 110/157\n",
      "Epoch: 1 | Step: 120/157\n",
      "Epoch: 1 | Step: 130/157\n",
      "Epoch: 1 | Step: 140/157\n",
      "Epoch: 1 | Step: 150/157\n",
      "Fold: 1 | Epoch: 1 | LR: 0.0000300 | Train loss: 0.01893 | Val loss: 0.00752 | Val AUC: 0.91975 | Val pAUC: 0.14259\n",
      "pAUC: 0.00000 --> 0.14259, saving model...\n",
      "Epoch 1 took 27m 27s\n",
      "Fold 1 | Epoch 2\n",
      "Epoch: 2 | Step: 1/5014 | Loss: 0.00327 | Smooth loss: 0.00327\n",
      "Epoch: 2 | Step: 100/5014 | Loss: 0.09581 | Smooth loss: 0.00244\n",
      "Epoch: 2 | Step: 200/5014 | Loss: 0.00084 | Smooth loss: 0.00591\n",
      "Epoch: 2 | Step: 300/5014 | Loss: 0.00076 | Smooth loss: 0.00677\n",
      "Epoch: 2 | Step: 400/5014 | Loss: 0.00085 | Smooth loss: 0.00742\n",
      "Epoch: 2 | Step: 500/5014 | Loss: 0.00119 | Smooth loss: 0.00586\n",
      "Epoch: 2 | Step: 600/5014 | Loss: 0.00073 | Smooth loss: 0.00449\n",
      "Epoch: 2 | Step: 700/5014 | Loss: 0.00073 | Smooth loss: 0.00762\n",
      "Epoch: 2 | Step: 800/5014 | Loss: 0.00095 | Smooth loss: 0.00498\n",
      "Epoch: 2 | Step: 900/5014 | Loss: 0.00111 | Smooth loss: 0.00857\n",
      "Epoch: 2 | Step: 1000/5014 | Loss: 0.00098 | Smooth loss: 0.00726\n",
      "Epoch: 2 | Step: 1100/5014 | Loss: 0.00089 | Smooth loss: 0.00661\n",
      "Epoch: 2 | Step: 1200/5014 | Loss: 0.00066 | Smooth loss: 0.00835\n",
      "Epoch: 2 | Step: 1300/5014 | Loss: 0.00084 | Smooth loss: 0.00504\n",
      "Epoch: 2 | Step: 1400/5014 | Loss: 0.00055 | Smooth loss: 0.00497\n",
      "Epoch: 2 | Step: 1500/5014 | Loss: 0.00069 | Smooth loss: 0.00618\n",
      "Epoch: 2 | Step: 1600/5014 | Loss: 0.00058 | Smooth loss: 0.00533\n",
      "Epoch: 2 | Step: 1700/5014 | Loss: 0.00056 | Smooth loss: 0.00433\n",
      "Epoch: 2 | Step: 1800/5014 | Loss: 0.00149 | Smooth loss: 0.00923\n",
      "Epoch: 2 | Step: 1900/5014 | Loss: 0.00158 | Smooth loss: 0.00815\n",
      "Epoch: 2 | Step: 2000/5014 | Loss: 0.00064 | Smooth loss: 0.00470\n",
      "Epoch: 2 | Step: 2100/5014 | Loss: 0.00122 | Smooth loss: 0.01078\n",
      "Epoch: 2 | Step: 2200/5014 | Loss: 0.00098 | Smooth loss: 0.00811\n",
      "Epoch: 2 | Step: 2300/5014 | Loss: 0.00094 | Smooth loss: 0.00821\n",
      "Epoch: 2 | Step: 2400/5014 | Loss: 0.00050 | Smooth loss: 0.00272\n",
      "Epoch: 2 | Step: 2500/5014 | Loss: 0.00040 | Smooth loss: 0.00350\n",
      "Epoch: 2 | Step: 2600/5014 | Loss: 0.00080 | Smooth loss: 0.00501\n",
      "Epoch: 2 | Step: 2700/5014 | Loss: 0.00087 | Smooth loss: 0.00675\n",
      "Epoch: 2 | Step: 2800/5014 | Loss: 0.00052 | Smooth loss: 0.00233\n",
      "Epoch: 2 | Step: 2900/5014 | Loss: 0.00087 | Smooth loss: 0.00656\n",
      "Epoch: 2 | Step: 3000/5014 | Loss: 0.00107 | Smooth loss: 0.00867\n",
      "Epoch: 2 | Step: 3100/5014 | Loss: 0.00236 | Smooth loss: 0.00767\n",
      "Epoch: 2 | Step: 3200/5014 | Loss: 0.00067 | Smooth loss: 0.00340\n",
      "Epoch: 2 | Step: 3300/5014 | Loss: 0.00080 | Smooth loss: 0.00607\n",
      "Epoch: 2 | Step: 3400/5014 | Loss: 0.00058 | Smooth loss: 0.00573\n",
      "Epoch: 2 | Step: 3500/5014 | Loss: 0.00076 | Smooth loss: 0.00558\n",
      "Epoch: 2 | Step: 3600/5014 | Loss: 0.00086 | Smooth loss: 0.00907\n",
      "Epoch: 2 | Step: 3700/5014 | Loss: 0.00114 | Smooth loss: 0.00807\n",
      "Epoch: 2 | Step: 3800/5014 | Loss: 0.00070 | Smooth loss: 0.01117\n",
      "Epoch: 2 | Step: 3900/5014 | Loss: 0.00073 | Smooth loss: 0.00317\n",
      "Epoch: 2 | Step: 4000/5014 | Loss: 0.00075 | Smooth loss: 0.00499\n",
      "Epoch: 2 | Step: 4100/5014 | Loss: 0.00085 | Smooth loss: 0.00813\n",
      "Epoch: 2 | Step: 4200/5014 | Loss: 0.00095 | Smooth loss: 0.00866\n",
      "Epoch: 2 | Step: 4300/5014 | Loss: 0.00163 | Smooth loss: 0.01098\n",
      "Epoch: 2 | Step: 4400/5014 | Loss: 0.00220 | Smooth loss: 0.00889\n",
      "Epoch: 2 | Step: 4500/5014 | Loss: 0.00109 | Smooth loss: 0.00687\n",
      "Epoch: 2 | Step: 4600/5014 | Loss: 0.00080 | Smooth loss: 0.00626\n",
      "Epoch: 2 | Step: 4700/5014 | Loss: 0.00161 | Smooth loss: 0.01061\n",
      "Epoch: 2 | Step: 4800/5014 | Loss: 0.00074 | Smooth loss: 0.00540\n",
      "Epoch: 2 | Step: 4900/5014 | Loss: 0.00112 | Smooth loss: 0.00837\n",
      "Epoch: 2 | Step: 5000/5014 | Loss: 0.00077 | Smooth loss: 0.00656\n",
      "Epoch: 2 | Step: 1/157\n",
      "Epoch: 2 | Step: 10/157\n",
      "Epoch: 2 | Step: 20/157\n",
      "Epoch: 2 | Step: 30/157\n",
      "Epoch: 2 | Step: 40/157\n",
      "Epoch: 2 | Step: 50/157\n",
      "Epoch: 2 | Step: 60/157\n",
      "Epoch: 2 | Step: 70/157\n",
      "Epoch: 2 | Step: 80/157\n",
      "Epoch: 2 | Step: 90/157\n",
      "Epoch: 2 | Step: 100/157\n",
      "Epoch: 2 | Step: 110/157\n",
      "Epoch: 2 | Step: 120/157\n",
      "Epoch: 2 | Step: 130/157\n",
      "Epoch: 2 | Step: 140/157\n",
      "Epoch: 2 | Step: 150/157\n",
      "Fold: 1 | Epoch: 2 | LR: 0.0003000 | Train loss: 0.00666 | Val loss: 0.00638 | Val AUC: 0.91948 | Val pAUC: 0.13655\n",
      "pAUC: 0.14259 --> 0.13655, skipping model save...\n",
      "Epoch 2 took 26m 27s\n",
      "Fold 1 | Epoch 3\n",
      "Epoch: 3 | Step: 1/5014 | Loss: 0.00076 | Smooth loss: 0.00076\n",
      "Epoch: 3 | Step: 100/5014 | Loss: 0.00148 | Smooth loss: 0.00858\n",
      "Epoch: 3 | Step: 200/5014 | Loss: 0.00066 | Smooth loss: 0.00659\n",
      "Epoch: 3 | Step: 300/5014 | Loss: 0.00070 | Smooth loss: 0.00466\n",
      "Epoch: 3 | Step: 400/5014 | Loss: 0.00048 | Smooth loss: 0.00339\n",
      "Epoch: 3 | Step: 500/5014 | Loss: 0.00050 | Smooth loss: 0.00711\n",
      "Epoch: 3 | Step: 600/5014 | Loss: 0.00079 | Smooth loss: 0.00766\n",
      "Epoch: 3 | Step: 700/5014 | Loss: 0.00064 | Smooth loss: 0.00544\n",
      "Epoch: 3 | Step: 800/5014 | Loss: 0.00061 | Smooth loss: 0.00659\n",
      "Epoch: 3 | Step: 900/5014 | Loss: 0.00049 | Smooth loss: 0.00397\n",
      "Epoch: 3 | Step: 1000/5014 | Loss: 0.00053 | Smooth loss: 0.00252\n",
      "Epoch: 3 | Step: 1100/5014 | Loss: 0.00058 | Smooth loss: 0.00294\n",
      "Epoch: 3 | Step: 1200/5014 | Loss: 0.00102 | Smooth loss: 0.00618\n",
      "Epoch: 3 | Step: 1300/5014 | Loss: 0.00253 | Smooth loss: 0.00796\n",
      "Epoch: 3 | Step: 1400/5014 | Loss: 0.00141 | Smooth loss: 0.00566\n",
      "Epoch: 3 | Step: 1500/5014 | Loss: 0.00094 | Smooth loss: 0.00402\n",
      "Epoch: 3 | Step: 1600/5014 | Loss: 0.00059 | Smooth loss: 0.00484\n",
      "Epoch: 3 | Step: 1700/5014 | Loss: 0.00040 | Smooth loss: 0.00406\n",
      "Epoch: 3 | Step: 1800/5014 | Loss: 0.00087 | Smooth loss: 0.00751\n",
      "Epoch: 3 | Step: 1900/5014 | Loss: 0.08091 | Smooth loss: 0.00510\n",
      "Epoch: 3 | Step: 2000/5014 | Loss: 0.00072 | Smooth loss: 0.00695\n",
      "Epoch: 3 | Step: 2100/5014 | Loss: 0.00164 | Smooth loss: 0.01216\n",
      "Epoch: 3 | Step: 2200/5014 | Loss: 0.00113 | Smooth loss: 0.00740\n",
      "Epoch: 3 | Step: 2300/5014 | Loss: 0.00049 | Smooth loss: 0.00201\n",
      "Epoch: 3 | Step: 2400/5014 | Loss: 0.00057 | Smooth loss: 0.00436\n",
      "Epoch: 3 | Step: 2500/5014 | Loss: 0.00220 | Smooth loss: 0.01388\n",
      "Epoch: 3 | Step: 2600/5014 | Loss: 0.00084 | Smooth loss: 0.00714\n",
      "Epoch: 3 | Step: 2700/5014 | Loss: 0.00096 | Smooth loss: 0.01142\n",
      "Epoch: 3 | Step: 2800/5014 | Loss: 0.00135 | Smooth loss: 0.00694\n",
      "Epoch: 3 | Step: 2900/5014 | Loss: 0.00203 | Smooth loss: 0.00897\n",
      "Epoch: 3 | Step: 3000/5014 | Loss: 0.00098 | Smooth loss: 0.00594\n",
      "Epoch: 3 | Step: 3100/5014 | Loss: 0.00205 | Smooth loss: 0.00774\n",
      "Epoch: 3 | Step: 3200/5014 | Loss: 0.00185 | Smooth loss: 0.00683\n",
      "Epoch: 3 | Step: 3300/5014 | Loss: 0.00127 | Smooth loss: 0.00424\n",
      "Epoch: 3 | Step: 3400/5014 | Loss: 0.00084 | Smooth loss: 0.00902\n",
      "Epoch: 3 | Step: 3500/5014 | Loss: 0.00122 | Smooth loss: 0.00541\n",
      "Epoch: 3 | Step: 3600/5014 | Loss: 0.00147 | Smooth loss: 0.00741\n",
      "Epoch: 3 | Step: 3700/5014 | Loss: 0.00116 | Smooth loss: 0.00684\n",
      "Epoch: 3 | Step: 3800/5014 | Loss: 0.00138 | Smooth loss: 0.00588\n",
      "Epoch: 3 | Step: 3900/5014 | Loss: 0.00106 | Smooth loss: 0.00849\n",
      "Epoch: 3 | Step: 4000/5014 | Loss: 0.00074 | Smooth loss: 0.00626\n",
      "Epoch: 3 | Step: 4100/5014 | Loss: 0.00086 | Smooth loss: 0.01008\n",
      "Epoch: 3 | Step: 4200/5014 | Loss: 0.00059 | Smooth loss: 0.00516\n",
      "Epoch: 3 | Step: 4300/5014 | Loss: 0.00093 | Smooth loss: 0.00317\n",
      "Epoch: 3 | Step: 4400/5014 | Loss: 0.00070 | Smooth loss: 0.00427\n",
      "Epoch: 3 | Step: 4500/5014 | Loss: 0.00104 | Smooth loss: 0.00925\n",
      "Epoch: 3 | Step: 4600/5014 | Loss: 0.00102 | Smooth loss: 0.00524\n",
      "Epoch: 3 | Step: 4700/5014 | Loss: 0.00077 | Smooth loss: 0.00686\n",
      "Epoch: 3 | Step: 4800/5014 | Loss: 0.00070 | Smooth loss: 0.00608\n",
      "Epoch: 3 | Step: 4900/5014 | Loss: 0.00140 | Smooth loss: 0.00958\n",
      "Epoch: 3 | Step: 5000/5014 | Loss: 0.00078 | Smooth loss: 0.00539\n",
      "Epoch: 3 | Step: 1/157\n",
      "Epoch: 3 | Step: 10/157\n",
      "Epoch: 3 | Step: 20/157\n",
      "Epoch: 3 | Step: 30/157\n",
      "Epoch: 3 | Step: 40/157\n",
      "Epoch: 3 | Step: 50/157\n",
      "Epoch: 3 | Step: 60/157\n",
      "Epoch: 3 | Step: 70/157\n",
      "Epoch: 3 | Step: 80/157\n",
      "Epoch: 3 | Step: 90/157\n",
      "Epoch: 3 | Step: 100/157\n",
      "Epoch: 3 | Step: 110/157\n",
      "Epoch: 3 | Step: 120/157\n",
      "Epoch: 3 | Step: 130/157\n",
      "Epoch: 3 | Step: 140/157\n",
      "Epoch: 3 | Step: 150/157\n",
      "Fold: 1 | Epoch: 3 | LR: 0.0002980 | Train loss: 0.00653 | Val loss: 0.00603 | Val AUC: 0.92937 | Val pAUC: 0.14953\n",
      "pAUC: 0.14259 --> 0.14953, saving model...\n",
      "Epoch 3 took 26m 46s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('efficientnet_b0_v3_finetune/models/fold_1/final')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ISICNet(\n",
    "    model_name=args.model_name\n",
    ")\n",
    "model = model.to(accelerator.device)\n",
    "tensors = {}\n",
    "with safe_open(f\"{args.pretrained_weights_path}/models/fold_{args.fold}/model.safetensors\", framework=\"pt\") as f:\n",
    "    for key in f.keys():\n",
    "        if \"classifier\" not in key:\n",
    "            tensors[key] = f.get_tensor(key)\n",
    "msg = model.load_state_dict(tensors, strict=False)\n",
    "print(msg)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args.init_lr, weight_decay=0.001, amsgrad=True)\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    pct_start=1 / args.num_epochs,\n",
    "    max_lr=args.init_lr * 10,\n",
    "    div_factor=10,\n",
    "    epochs=args.num_epochs,\n",
    "    steps_per_epoch=len(dev_dataloader),\n",
    ")\n",
    "(\n",
    "    model,\n",
    "    optimizer,\n",
    "    dev_dataloader,\n",
    "    val_dataloader,\n",
    "    lr_scheduler,\n",
    ") = accelerator.prepare(\n",
    "    model, optimizer, dev_dataloader, val_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "best_val_auc = 0\n",
    "best_val_pauc = 0\n",
    "best_val_loss = 0\n",
    "best_epoch = 0\n",
    "best_val_probs = None\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_paucs = []\n",
    "val_aucs = []\n",
    "for epoch in range(1, args.num_epochs + 1):\n",
    "    print(f\"Fold {args.fold} | Epoch {epoch}\")\n",
    "    start_time = time.time()\n",
    "    lr = optimizer.param_groups[0][\"lr\"]\n",
    "    train_loss = train_epoch(\n",
    "        epoch,\n",
    "        model,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        dev_dataloader,\n",
    "        lr_scheduler,\n",
    "        accelerator,\n",
    "    )\n",
    "    (\n",
    "        val_loss,\n",
    "        val_auc,\n",
    "        val_pauc,\n",
    "        val_probs,\n",
    "        val_targets,\n",
    "    ) = val_epoch(\n",
    "        epoch,\n",
    "        model,\n",
    "        criterion,\n",
    "        val_dataloader,\n",
    "        accelerator,\n",
    "        args.n_tta,\n",
    "    )\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    val_paucs.append(val_pauc)\n",
    "    val_aucs.append(val_auc)\n",
    "    print(\n",
    "        f\"Fold: {args.fold} | Epoch: {epoch} | LR: {lr:.7f} |\"\n",
    "        f\" Train loss: {train_loss:.5f} | Val loss: {val_loss:.5f} |\"\n",
    "        f\" Val AUC: {val_auc:.5f} | Val pAUC: {val_pauc:.5f}\"\n",
    "    )\n",
    "    if val_pauc > best_val_pauc:\n",
    "        print(\n",
    "            f\"pAUC: {best_val_pauc:.5f} --> {val_pauc:.5f}, saving model...\"\n",
    "        )\n",
    "        best_val_pauc = val_pauc\n",
    "        best_val_auc = val_auc\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "        best_val_probs = val_probs\n",
    "        output_dir = f\"{args.model_dir}/models/fold_{args.fold}\"\n",
    "        accelerator.save_state(output_dir)\n",
    "    else:\n",
    "        print(\n",
    "            f\"pAUC: {best_val_pauc:.5f} --> {val_pauc:.5f}, skipping model save...\"\n",
    "        )\n",
    "    elapsed_time = time.time() - start_time\n",
    "    elapsed_mins = int(elapsed_time // 60)\n",
    "    elapsed_secs = int(elapsed_time % 60)\n",
    "    print(f\"Epoch {epoch} took {elapsed_mins}m {elapsed_secs}s\")\n",
    "    if epoch == 3:\n",
    "        break\n",
    "\n",
    "output_dir = f\"{args.model_dir}/models/fold_{args.fold}/final\"\n",
    "accelerator.save_state(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b723c776",
   "metadata": {
    "papermill": {
     "duration": 0.01926,
     "end_time": "2024-08-13T21:16:52.618442",
     "exception": false,
     "start_time": "2024-08-13T21:16:52.599182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9094797,
     "sourceId": 63056,
     "sourceType": "competition"
    },
    {
     "datasetId": 5521264,
     "sourceId": 9141639,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 189656082,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 187477024,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4868.691893,
   "end_time": "2024-08-13T21:16:55.809417",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-13T19:55:47.117524",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
