{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install libauc","metadata":{"execution":{"iopub.status.busy":"2024-08-13T19:58:41.553199Z","iopub.execute_input":"2024-08-13T19:58:41.553663Z","iopub.status.idle":"2024-08-13T19:58:57.520006Z","shell.execute_reply.started":"2024-08-13T19:58:41.553618Z","shell.execute_reply":"2024-08-13T19:58:57.518799Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Collecting libauc\n  Downloading libauc-1.4.0-py3-none-any.whl.metadata (8.1 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from libauc) (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from libauc) (0.16.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from libauc) (1.26.4)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from libauc) (4.66.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from libauc) (1.11.4)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from libauc) (2.2.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from libauc) (9.5.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from libauc) (1.2.2)\nRequirement already satisfied: opencv-python in /opt/conda/lib/python3.10/site-packages (from libauc) (4.10.0.84)\nCollecting torch-geometric (from libauc)\n  Downloading torch_geometric-2.5.3-py3-none-any.whl.metadata (64 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting ogb (from libauc)\n  Downloading ogb-1.3.6-py3-none-any.whl.metadata (6.2 kB)\nCollecting webdataset (from libauc)\n  Downloading webdataset-0.2.96-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from ogb->libauc) (1.16.0)\nRequirement already satisfied: urllib3>=1.24.0 in /opt/conda/lib/python3.10/site-packages (from ogb->libauc) (1.26.18)\nCollecting outdated>=0.2.0 (from ogb->libauc)\n  Downloading outdated-0.2.2-py2.py3-none-any.whl.metadata (4.7 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->libauc) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->libauc) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->libauc) (2023.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->libauc) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->libauc) (3.2.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->libauc) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->libauc) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->libauc) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->libauc) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->libauc) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->libauc) (2024.5.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from torch-geometric->libauc) (3.9.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch-geometric->libauc) (2.32.3)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch-geometric->libauc) (3.1.1)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch-geometric->libauc) (5.9.3)\nCollecting braceexpand (from webdataset->libauc)\n  Downloading braceexpand-0.1.7-py2.py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from webdataset->libauc) (6.0.1)\nRequirement already satisfied: setuptools>=44 in /opt/conda/lib/python3.10/site-packages (from outdated>=0.2.0->ogb->libauc) (69.0.3)\nCollecting littleutils (from outdated>=0.2.0->ogb->libauc)\n  Downloading littleutils-0.2.4-py3-none-any.whl.metadata (679 bytes)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric->libauc) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric->libauc) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric->libauc) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric->libauc) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric->libauc) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric->libauc) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->libauc) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric->libauc) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric->libauc) (3.6)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric->libauc) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->libauc) (1.3.0)\nDownloading libauc-1.4.0-py3-none-any.whl (130 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.8/130.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ogb-1.3.6-py3-none-any.whl (78 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading webdataset-0.2.96-py3-none-any.whl (74 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.8/74.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\nDownloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\nDownloading littleutils-0.2.4-py3-none-any.whl (8.1 kB)\nInstalling collected packages: braceexpand, webdataset, littleutils, outdated, torch-geometric, ogb, libauc\nSuccessfully installed braceexpand-0.1.7 libauc-1.4.0 littleutils-0.2.4 ogb-1.3.6 outdated-0.2.2 torch-geometric-2.5.3 webdataset-0.2.96\n","output_type":"stream"}]},{"cell_type":"code","source":"from libauc.losses.auc import pAUC_DRO_Loss\nfrom libauc.optimizers import SOPAs\nfrom libauc.sampler import DualSampler","metadata":{"execution":{"iopub.status.busy":"2024-08-13T20:37:50.460539Z","iopub.execute_input":"2024-08-13T20:37:50.461437Z","iopub.status.idle":"2024-08-13T20:37:50.466270Z","shell.execute_reply.started":"2024-08-13T20:37:50.461392Z","shell.execute_reply":"2024-08-13T20:37:50.465224Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"import argparse\nimport json\nimport logging\nimport time\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom accelerate import Accelerator\nfrom accelerate.utils import (\n    DistributedDataParallelKwargs,\n    ProjectConfiguration,\n    set_seed,\n)\nfrom accelerate.logging import get_logger\nfrom io import BytesIO\n\nimport albumentations as A\nimport h5py\nfrom albumentations.pytorch import ToTensorV2\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler\nfrom timm import create_model\nimport torch.nn.functional as F\nfrom safetensors import safe_open\n\nfrom dataclasses import dataclass\nfrom isic_helper import get_folds\nfrom isic_helper import compute_auc, compute_pauc","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-13T19:52:20.606280Z","iopub.execute_input":"2024-08-13T19:52:20.607047Z","iopub.status.idle":"2024-08-13T19:52:28.501310Z","shell.execute_reply.started":"2024-08-13T19:52:20.606993Z","shell.execute_reply":"2024-08-13T19:52:28.500332Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"@dataclass\nclass Config:\n    mixed_precision: bool = \"fp16\"\n    image_size: int = 64\n    train_batch_size: int = 64\n    val_batch_size: int = 512\n    num_workers: int = 4\n    init_lr: float = 3e-5\n    num_epochs: int = 20\n    n_tta: int = 8\n    seed: int = 2022\n\n    ext: str = \"2020,2019\"\n    only_malignant: bool = False\n    debug: bool = False\n\nargs = Config()\nargs.model_name = \"efficientnet_b0\"\nargs.version = \"v3\"\nargs.model_identifier = f\"{args.model_name}_{args.version}\"\nargs.pretrained_weights_path = f\"/kaggle/input/isic-scd-{args.model_name.replace('_', '-')}-{args.version}-train\"\nargs.fold = 1\nargs.model_dir = f\"{args.model_identifier}_finetune\"\nargs.logging_dir = \"logs\"","metadata":{"execution":{"iopub.status.busy":"2024-08-13T20:44:35.369172Z","iopub.execute_input":"2024-08-13T20:44:35.369919Z","iopub.status.idle":"2024-08-13T20:44:35.379478Z","shell.execute_reply.started":"2024-08-13T20:44:35.369881Z","shell.execute_reply":"2024-08-13T20:44:35.378454Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"def dev_augment(image_size, mean=None, std=None):\n    if mean is not None and std is not None:\n        normalize = A.Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0)\n    else:\n        normalize = A.Normalize(max_pixel_value=255.0, p=1.0)\n    transform = A.Compose(\n        [\n            A.Transpose(p=0.5),\n            A.VerticalFlip(p=0.5),\n            A.HorizontalFlip(p=0.5),\n            A.RandomBrightnessContrast(\n                brightness_limit=0.2, contrast_limit=0.2, p=0.75\n            ),\n            A.OneOf(\n                [\n                    A.MotionBlur(blur_limit=(5, 7)),\n                    A.MedianBlur(blur_limit=(5, 7)),\n                    A.GaussianBlur(blur_limit=(5, 7)),\n                    A.GaussNoise(var_limit=(5.0, 30.0)),\n                ],\n                p=0.7,\n            ),\n            A.OneOf(\n                [\n                    A.OpticalDistortion(distort_limit=1.0),\n                    A.GridDistortion(num_steps=5, distort_limit=1.0),\n                    A.ElasticTransform(alpha=3),\n                ],\n                p=0.7,\n            ),\n            A.CLAHE(clip_limit=4.0, p=0.7),\n            A.HueSaturationValue(\n                hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5\n            ),\n            A.ShiftScaleRotate(\n                shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.85\n            ),\n            A.Resize(image_size, image_size),\n            A.CoarseDropout(\n                max_height=int(image_size * 0.375),\n                max_width=int(image_size * 0.375),\n                max_holes=1,\n                min_holes=1,\n                p=0.7,\n            ),\n            normalize,\n            ToTensorV2(),\n        ],\n        p=1.0,\n    )\n    return transform\n\n\ndef val_augment(image_size, mean=None, std=None):\n    if mean is not None and std is not None:\n        normalize = A.Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0)\n    else:\n        normalize = A.Normalize(max_pixel_value=255.0, p=1.0)\n    transform = A.Compose(\n        [A.Resize(image_size, image_size), normalize, ToTensorV2()], p=1.0\n    )\n    return transform\n\n\nclass ISICDataset(Dataset):\n    def __init__(self, metadata, images, augment, infer=False):\n        self.metadata = metadata\n        self.images = images\n        self.augment = augment\n        self.length = len(self.metadata)\n        self.infer = infer\n        if not infer:\n            self.targets = self.metadata.target\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, index):\n        row = self.metadata.iloc[index]\n        image = np.array(Image.open(BytesIO(self.images[row[\"isic_id\"]][()])))\n        if self.augment is not None:\n            image = self.augment(image=image)[\"image\"].float()\n        if self.infer:\n            return image, index\n        else:\n            target = torch.tensor(row[\"target\"])\n            return image, target, index\n\nclass ISICNet(nn.Module):\n    def __init__(\n        self,\n        model_name\n    ):\n        super(ISICNet, self).__init__()\n        self.model = create_model(\n            model_name=model_name,\n            pretrained=False,\n            in_chans=3,\n            num_classes=0,\n            global_pool=\"\",\n        )\n        in_dim = self.model.num_features\n        self.classifier = nn.Linear(in_dim, 1)\n        self.dropouts = nn.ModuleList([nn.Dropout(0.5) for _ in range(5)])\n\n    def forward(self, images):\n        x = self.model(images)\n        bs = len(images)\n        pool = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)\n        if self.training:\n            logits = 0\n            for i in range(len(self.dropouts)):\n                logits += self.classifier(self.dropouts[i](pool))\n            logits = logits / len(self.dropouts)\n        else:\n            logits = self.classifier(pool)\n        return logits\n\ndef train_epoch(\n    epoch,\n    model,\n    optimizer,\n    criterion,\n    dev_dataloader,\n    lr_scheduler,\n    accelerator,\n    log_interval=100,\n):\n    model.train()\n    train_loss = []\n    total_steps = len(dev_dataloader)\n    for step, (images, targets, index) in enumerate(dev_dataloader):\n        optimizer.zero_grad()\n        logits = model(images)\n        probs = torch.sigmoid(logits)\n        targets = targets.float().unsqueeze(1)\n        loss = criterion(probs, targets, index)\n        accelerator.backward(loss)\n        optimizer.step()\n        lr_scheduler.step()\n\n        loss_value = accelerator.gather(loss).item()\n        train_loss.append(loss_value)\n        smooth_loss = sum(train_loss[-100:]) / min(len(train_loss), 100)\n        if (step == 0) or ((step + 1) % log_interval == 0):\n            print(\n                f\"Epoch: {epoch} | Step: {step + 1}/{total_steps} |\"\n                f\" Loss: {loss_value:.5f} | Smooth loss: {smooth_loss:.5f}\"\n            )\n    train_loss = np.mean(train_loss)\n    return train_loss\n\n\ndef get_trans(img, iteration):\n    if iteration >= 6:\n        img = img.transpose(2, 3)\n    if iteration % 6 == 0:\n        return img\n    elif iteration % 6 == 1:\n        return torch.flip(img, dims=[2])\n    elif iteration % 6 == 2:\n        return torch.flip(img, dims=[3])\n    elif iteration % 6 == 3:\n        return torch.rot90(img, 1, dims=[2, 3])\n    elif iteration % 6 == 4:\n        return torch.rot90(img, 2, dims=[2, 3])\n    elif iteration % 6 == 5:\n        return torch.rot90(img, 3, dims=[2, 3])\n\n\ndef val_epoch(\n    epoch,\n    model,\n    criterion,\n    val_dataloader,\n    accelerator,\n    n_tta,\n    log_interval=10,\n):\n    model.eval()\n    val_probs = []\n    val_targets = []\n    val_loss = []\n    total_steps = len(val_dataloader)\n    with torch.no_grad():\n        for step, (images, targets, index) in enumerate(val_dataloader):\n            logits = 0\n            probs = 0\n            for i in range(n_tta):\n                logits_iter = model(get_trans(images, i))\n                logits += logits_iter\n                probs += torch.sigmoid(logits_iter)\n            logits /= n_tta\n            probs /= n_tta\n\n            targets = targets.float().unsqueeze(1)\n#             loss = criterion(probs, targets, index)\n#             val_loss.append(loss.detach().cpu().numpy())\n\n            probs, targets = accelerator.gather((probs, targets))\n            val_probs.append(probs)\n            val_targets.append(targets)\n\n            if (step == 0) or ((step + 1) % log_interval == 0):\n                print(f\"Epoch: {epoch} | Step: {step + 1}/{total_steps}\")\n\n    val_loss = np.mean(val_loss)\n    val_probs = torch.cat(val_probs).cpu().numpy()\n    val_targets = torch.cat(val_targets).cpu().numpy()\n    val_auc = compute_auc(val_targets, val_probs)\n    val_pauc = compute_pauc(val_targets, val_probs, min_tpr=0.8)\n    return (\n        val_loss,\n        val_auc,\n        val_pauc,\n        val_probs,\n        val_targets,\n    )\n","metadata":{"execution":{"iopub.status.busy":"2024-08-13T21:15:17.762086Z","iopub.execute_input":"2024-08-13T21:15:17.762627Z","iopub.status.idle":"2024-08-13T21:15:17.801207Z","shell.execute_reply.started":"2024-08-13T21:15:17.762582Z","shell.execute_reply":"2024-08-13T21:15:17.800188Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"logger = get_logger(__name__)\nlogging_dir = Path(args.model_dir, args.logging_dir)\naccelerator_project_config = ProjectConfiguration(\n    project_dir=args.model_dir, logging_dir=str(logging_dir)\n)\nkwargs = DistributedDataParallelKwargs()\naccelerator = Accelerator(\n    mixed_precision=args.mixed_precision,\n    project_config=accelerator_project_config,\n    kwargs_handlers=[kwargs],\n)\n\nlogging.basicConfig(\n    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n    datefmt=\"%m/%d/%Y %H:%M:%S\",\n    level=logging.INFO,\n)\nprint(accelerator.state)\n\nif args.seed is not None:\n    set_seed(args.seed)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T19:54:33.050743Z","iopub.execute_input":"2024-08-13T19:54:33.051412Z","iopub.status.idle":"2024-08-13T19:54:33.061585Z","shell.execute_reply.started":"2024-08-13T19:54:33.051373Z","shell.execute_reply":"2024-08-13T19:54:33.060231Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Distributed environment: NO\nNum processes: 1\nProcess index: 0\nLocal process index: 0\nDevice: cuda\n\nMixed precision type: fp16\n\n","output_type":"stream"}]},{"cell_type":"code","source":"id_column = \"isic_id\"\ntarget_column = \"target\"\ngroup_column = \"patient_id\"\n\nINPUT_PATH = Path(\"/kaggle/input/isic-2024-challenge/\")\n\ntrain_metadata = pd.read_csv(INPUT_PATH / \"train-metadata.csv\", low_memory=False, na_values=[\"NA\"])\ntest_metadata = pd.read_csv(INPUT_PATH / \"test-metadata.csv\", low_memory=False, na_values=[\"NA\"])\n\nfolds_df = get_folds()\ntrain_metadata = train_metadata.merge(folds_df, on=[\"isic_id\", \"patient_id\"], how=\"inner\")\nprint(f\"Train data size: {train_metadata.shape}\")\nprint(f\"Test data size: {test_metadata.shape}\")\n\ntrain_images = h5py.File(INPUT_PATH / \"train-image.hdf5\", mode=\"r\")","metadata":{"execution":{"iopub.status.busy":"2024-08-13T19:54:33.350816Z","iopub.execute_input":"2024-08-13T19:54:33.351223Z","iopub.status.idle":"2024-08-13T19:54:41.237725Z","shell.execute_reply.started":"2024-08-13T19:54:33.351193Z","shell.execute_reply":"2024-08-13T19:54:41.236708Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Train data size: (401059, 57)\nTest data size: (3, 44)\n","output_type":"stream"}]},{"cell_type":"code","source":"if args.debug:\n    args.num_epochs = 2\n    dev_index = (\n        train_metadata[train_metadata[\"fold\"] != args.fold]\n        .sample(args.train_batch_size * 3, random_state=args.seed)\n        .index\n    )\n    val_index = (\n        train_metadata[train_metadata[\"fold\"] == args.fold]\n        .sample(args.val_batch_size * 10, random_state=args.seed)\n        .index\n    )\nelse:\n    dev_index = train_metadata[train_metadata[\"fold\"] != args.fold].index\n    val_index = train_metadata[train_metadata[\"fold\"] == args.fold].index\n\ndev_metadata = train_metadata.loc[dev_index, :].reset_index(drop=True)\nval_metadata = train_metadata.loc[val_index, :].reset_index(drop=True)\n\nmean = None\nstd = None\n\ndev_dataset = ISICDataset(\n    dev_metadata,\n    train_images,\n    augment=dev_augment(args.image_size, mean=mean, std=std),\n    infer=False,\n)\nval_dataset = ISICDataset(\n    val_metadata,\n    train_images,\n    augment=val_augment(args.image_size, mean=mean, std=std),\n    infer=False,\n)\n\n# sampler = RandomSampler(dev_dataset)\nsampler = DualSampler(dev_dataset, args.train_batch_size, sampling_rate=0.01)\n\ndev_dataloader = DataLoader(\n    dev_dataset,\n    batch_size=args.train_batch_size,\n    sampler=sampler,\n    num_workers=args.num_workers,\n    pin_memory=True,\n)\nval_dataloader = DataLoader(\n    val_dataset,\n    batch_size=args.val_batch_size,\n    shuffle=False,\n    num_workers=args.num_workers,\n    drop_last=False,\n    pin_memory=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T20:46:17.622522Z","iopub.execute_input":"2024-08-13T20:46:17.623266Z","iopub.status.idle":"2024-08-13T20:46:18.497898Z","shell.execute_reply.started":"2024-08-13T20:46:17.623225Z","shell.execute_reply":"2024-08-13T20:46:18.496696Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"model = ISICNet(\n    model_name=args.model_name\n)\nmodel = model.to(accelerator.device)\ntensors = {}\nwith safe_open(f\"{args.pretrained_weights_path}/models/fold_{args.fold}/model.safetensors\", framework=\"pt\") as f:\n    for key in f.keys():\n        if \"classifier\" not in key:\n            tensors[key] = f.get_tensor(key)\nmsg = model.load_state_dict(tensors, strict=False)\nprint(msg)\n# criterion = nn.BCELoss()\n# optimizer = torch.optim.AdamW(model.parameters(), lr=args.init_lr, weight_decay=0.001, amsgrad=True)\ncriterion = pAUC_DRO_Loss(data_len=len(dev_dataset), gamma=0.1, margin=1.0, Lambda=1.0)\noptimizer = SOPAs(model.parameters(), lr=args.init_lr, mode=\"adam\")\nlr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    pct_start=1 / args.num_epochs,\n    max_lr=args.init_lr * 10,\n    div_factor=10,\n    epochs=args.num_epochs,\n    steps_per_epoch=len(dev_dataloader),\n)\n(\n    model,\n    optimizer,\n    dev_dataloader,\n    val_dataloader,\n    lr_scheduler,\n) = accelerator.prepare(\n    model, optimizer, dev_dataloader, val_dataloader, lr_scheduler\n)\n\nbest_val_auc = 0\nbest_val_pauc = 0\nbest_val_loss = 0\nbest_epoch = 0\nbest_val_probs = None\ntrain_losses = []\nval_losses = []\nval_paucs = []\nval_aucs = []\nfor epoch in range(1, args.num_epochs + 1):\n    print(f\"Fold {args.fold} | Epoch {epoch}\")\n    start_time = time.time()\n    lr = optimizer.param_groups[0][\"lr\"]\n    train_loss = train_epoch(\n        epoch,\n        model,\n        optimizer,\n        criterion,\n        dev_dataloader,\n        lr_scheduler,\n        accelerator,\n    )\n    (\n        val_loss,\n        val_auc,\n        val_pauc,\n        val_probs,\n        val_targets,\n    ) = val_epoch(\n        epoch,\n        model,\n        criterion,\n        val_dataloader,\n        accelerator,\n        args.n_tta,\n    )\n    train_losses.append(train_loss)\n    val_losses.append(val_loss)\n    val_paucs.append(val_pauc)\n    val_aucs.append(val_auc)\n    print(\n        f\"Fold: {args.fold} | Epoch: {epoch} | LR: {lr:.7f} |\"\n        f\" Train loss: {train_loss:.5f} | Val loss: {val_loss:.5f} |\"\n        f\" Val AUC: {val_auc:.5f} | Val pAUC: {val_pauc:.5f}\"\n    )\n    if val_pauc > best_val_pauc:\n        print(\n            f\"pAUC: {best_val_pauc:.5f} --> {val_pauc:.5f}, saving model...\"\n        )\n        best_val_pauc = val_pauc\n        best_val_auc = val_auc\n        best_val_loss = val_loss\n        best_epoch = epoch\n        best_val_probs = val_probs\n        output_dir = f\"{args.model_dir}/models/fold_{args.fold}\"\n        accelerator.save_state(output_dir)\n    else:\n        print(\n            f\"pAUC: {best_val_pauc:.5f} --> {val_pauc:.5f}, skipping model save...\"\n        )\n    elapsed_time = time.time() - start_time\n    elapsed_mins = int(elapsed_time // 60)\n    elapsed_secs = int(elapsed_time % 60)\n    print(f\"Epoch {epoch} took {elapsed_mins}m {elapsed_secs}s\")\n    if epoch == 3:\n        break\n\noutput_dir = f\"{args.model_dir}/models/fold_{args.fold}/final\"\naccelerator.save_state(output_dir)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-13T20:46:18.499598Z","iopub.execute_input":"2024-08-13T20:46:18.499966Z","iopub.status.idle":"2024-08-13T21:13:59.104003Z","shell.execute_reply.started":"2024-08-13T20:46:18.499938Z","shell.execute_reply":"2024-08-13T21:13:59.102069Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"_IncompatibleKeys(missing_keys=['classifier.weight', 'classifier.bias'], unexpected_keys=[])\nFold 1 | Epoch 1\nEpoch: 1 | Step: 1/5087 | Loss: 10.26609 | Smooth loss: 10.26609\nEpoch: 1 | Step: 100/5087 | Loss: 6.58629 | Smooth loss: 9.03762\nEpoch: 1 | Step: 200/5087 | Loss: 4.54097 | Smooth loss: 7.19619\nEpoch: 1 | Step: 300/5087 | Loss: 3.62273 | Smooth loss: 6.35319\nEpoch: 1 | Step: 400/5087 | Loss: 1.80703 | Smooth loss: 3.36250\nEpoch: 1 | Step: 500/5087 | Loss: 1.84827 | Smooth loss: 2.81617\nEpoch: 1 | Step: 600/5087 | Loss: 8.64451 | Smooth loss: 2.65036\nEpoch: 1 | Step: 700/5087 | Loss: 0.91355 | Smooth loss: 1.76759\nEpoch: 1 | Step: 800/5087 | Loss: 2.02296 | Smooth loss: 1.85123\nEpoch: 1 | Step: 900/5087 | Loss: 0.45789 | Smooth loss: 2.11034\nEpoch: 1 | Step: 1000/5087 | Loss: 0.72213 | Smooth loss: 1.71044\nEpoch: 1 | Step: 1100/5087 | Loss: 5.31900 | Smooth loss: 1.75506\nEpoch: 1 | Step: 1200/5087 | Loss: 0.36238 | Smooth loss: 1.66128\nEpoch: 1 | Step: 1300/5087 | Loss: 5.57108 | Smooth loss: 1.58880\nEpoch: 1 | Step: 1400/5087 | Loss: 0.37121 | Smooth loss: 1.20874\nEpoch: 1 | Step: 1500/5087 | Loss: 0.39476 | Smooth loss: 1.52383\nEpoch: 1 | Step: 1600/5087 | Loss: 0.24138 | Smooth loss: 1.36822\nEpoch: 1 | Step: 1700/5087 | Loss: 0.79155 | Smooth loss: 1.22040\nEpoch: 1 | Step: 1800/5087 | Loss: 0.64762 | Smooth loss: 1.17063\nEpoch: 1 | Step: 1900/5087 | Loss: 0.39867 | Smooth loss: 1.18653\nEpoch: 1 | Step: 2000/5087 | Loss: 0.61850 | Smooth loss: 1.11053\nEpoch: 1 | Step: 2100/5087 | Loss: 0.23779 | Smooth loss: 1.02972\nEpoch: 1 | Step: 2200/5087 | Loss: 0.59296 | Smooth loss: 0.93899\nEpoch: 1 | Step: 2300/5087 | Loss: 0.98129 | Smooth loss: 1.10979\nEpoch: 1 | Step: 2400/5087 | Loss: 0.18506 | Smooth loss: 1.00500\nEpoch: 1 | Step: 2500/5087 | Loss: 0.53495 | Smooth loss: 0.99647\nEpoch: 1 | Step: 2600/5087 | Loss: 0.16900 | Smooth loss: 0.93583\nEpoch: 1 | Step: 2700/5087 | Loss: 0.19787 | Smooth loss: 0.80186\nEpoch: 1 | Step: 2800/5087 | Loss: 1.12571 | Smooth loss: 0.92187\nEpoch: 1 | Step: 2900/5087 | Loss: 0.22110 | Smooth loss: 0.97986\nEpoch: 1 | Step: 3000/5087 | Loss: 4.85393 | Smooth loss: 0.82942\nEpoch: 1 | Step: 3100/5087 | Loss: 0.16854 | Smooth loss: 0.76913\nEpoch: 1 | Step: 3200/5087 | Loss: 0.51363 | Smooth loss: 0.95022\nEpoch: 1 | Step: 3300/5087 | Loss: 0.72830 | Smooth loss: 0.98214\nEpoch: 1 | Step: 3400/5087 | Loss: 0.12048 | Smooth loss: 0.84753\nEpoch: 1 | Step: 3500/5087 | Loss: 3.11331 | Smooth loss: 0.83982\nEpoch: 1 | Step: 3600/5087 | Loss: 1.20755 | Smooth loss: 0.76939\nEpoch: 1 | Step: 3700/5087 | Loss: 0.15390 | Smooth loss: 0.78536\nEpoch: 1 | Step: 3800/5087 | Loss: 2.33011 | Smooth loss: 0.74265\nEpoch: 1 | Step: 3900/5087 | Loss: 6.87437 | Smooth loss: 0.84192\nEpoch: 1 | Step: 4000/5087 | Loss: 0.63334 | Smooth loss: 0.81696\nEpoch: 1 | Step: 4100/5087 | Loss: 0.68846 | Smooth loss: 0.65069\nEpoch: 1 | Step: 4200/5087 | Loss: 0.18600 | Smooth loss: 0.96348\nEpoch: 1 | Step: 4300/5087 | Loss: 0.83606 | Smooth loss: 0.91038\nEpoch: 1 | Step: 4400/5087 | Loss: 0.22654 | Smooth loss: 0.83312\nEpoch: 1 | Step: 4500/5087 | Loss: 0.80903 | Smooth loss: 0.72028\nEpoch: 1 | Step: 4600/5087 | Loss: 0.64046 | Smooth loss: 0.62880\nEpoch: 1 | Step: 4700/5087 | Loss: 0.28026 | Smooth loss: 0.71303\nEpoch: 1 | Step: 4800/5087 | Loss: 1.02766 | Smooth loss: 0.63735\nEpoch: 1 | Step: 4900/5087 | Loss: 0.19627 | Smooth loss: 0.87615\nEpoch: 1 | Step: 5000/5087 | Loss: 0.15618 | Smooth loss: 0.55644\nEpoch: 1 | Step: 1/157\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[47], line 62\u001b[0m\n\u001b[1;32m     46\u001b[0m lr \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     47\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_epoch(\n\u001b[1;32m     48\u001b[0m     epoch,\n\u001b[1;32m     49\u001b[0m     model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m     accelerator,\n\u001b[1;32m     55\u001b[0m )\n\u001b[1;32m     56\u001b[0m (\n\u001b[1;32m     57\u001b[0m     val_loss,\n\u001b[1;32m     58\u001b[0m     val_auc,\n\u001b[1;32m     59\u001b[0m     val_pauc,\n\u001b[1;32m     60\u001b[0m     val_probs,\n\u001b[1;32m     61\u001b[0m     val_targets,\n\u001b[0;32m---> 62\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mval_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_tta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m     71\u001b[0m val_losses\u001b[38;5;241m.\u001b[39mappend(val_loss)\n","Cell \u001b[0;32mIn[43], line 197\u001b[0m, in \u001b[0;36mval_epoch\u001b[0;34m(epoch, model, criterion, val_dataloader, accelerator, n_tta, log_interval)\u001b[0m\n\u001b[1;32m    194\u001b[0m probs \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m n_tta\n\u001b[1;32m    196\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 197\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m val_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    200\u001b[0m probs, targets \u001b[38;5;241m=\u001b[39m accelerator\u001b[38;5;241m.\u001b[39mgather((probs, targets))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/libauc/losses/auc.py:514\u001b[0m, in \u001b[0;36mpAUC_DRO_Loss.forward\u001b[0;34m(self, y_pred, y_true, index, **kwargs)\u001b[0m\n\u001b[1;32m    512\u001b[0m pos_mask \u001b[38;5;241m=\u001b[39m (y_true \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze() \n\u001b[1;32m    513\u001b[0m neg_mask \u001b[38;5;241m=\u001b[39m (y_true \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze() \n\u001b[0;32m--> 514\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28msum\u001b[39m(pos_mask) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput data has no positive sample! Please use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlibauc.sampler.DualSampler\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for data resampling!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(index) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_pred): \n\u001b[1;32m    516\u001b[0m     index \u001b[38;5;241m=\u001b[39m index[pos_mask]   \u001b[38;5;66;03m# indices for positive samples only       \u001b[39;00m\n","\u001b[0;31mAssertionError\u001b[0m: Input data has no positive sample! Please use 'libauc.sampler.DualSampler' for data resampling!"],"ename":"AssertionError","evalue":"Input data has no positive sample! Please use 'libauc.sampler.DualSampler' for data resampling!","output_type":"error"}]},{"cell_type":"code","source":"(\n    val_loss,\n    val_auc,\n    val_pauc,\n    val_probs,\n    val_targets,\n) = val_epoch(\n    epoch,\n    model,\n    criterion,\n    val_dataloader,\n    accelerator,\n    args.n_tta,\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T21:15:34.542862Z","iopub.execute_input":"2024-08-13T21:15:34.543752Z","iopub.status.idle":"2024-08-13T21:16:34.875428Z","shell.execute_reply.started":"2024-08-13T21:15:34.543716Z","shell.execute_reply":"2024-08-13T21:16:34.874169Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"Epoch: 1 | Step: 1/157\nEpoch: 1 | Step: 10/157\nEpoch: 1 | Step: 20/157\nEpoch: 1 | Step: 30/157\nEpoch: 1 | Step: 40/157\nEpoch: 1 | Step: 50/157\nEpoch: 1 | Step: 60/157\nEpoch: 1 | Step: 70/157\nEpoch: 1 | Step: 80/157\nEpoch: 1 | Step: 90/157\nEpoch: 1 | Step: 100/157\nEpoch: 1 | Step: 110/157\nEpoch: 1 | Step: 120/157\nEpoch: 1 | Step: 130/157\nEpoch: 1 | Step: 140/157\nEpoch: 1 | Step: 150/157\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n  return _methods._mean(a, axis=axis, dtype=dtype,\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n  ret = ret.dtype.type(ret / rcount)\n","output_type":"stream"}]},{"cell_type":"code","source":"val_pauc","metadata":{"execution":{"iopub.status.busy":"2024-08-13T21:16:45.839138Z","iopub.execute_input":"2024-08-13T21:16:45.839542Z","iopub.status.idle":"2024-08-13T21:16:45.847847Z","shell.execute_reply.started":"2024-08-13T21:16:45.839502Z","shell.execute_reply":"2024-08-13T21:16:45.846959Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"0.16481522146570746"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}