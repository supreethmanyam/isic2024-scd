{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":63056,"databundleVersionId":9094797,"sourceType":"competition"},{"sourceId":9084133,"sourceType":"datasetVersion","datasetId":5480989},{"sourceId":9084147,"sourceType":"datasetVersion","datasetId":5480997},{"sourceId":187477024,"sourceType":"kernelVersion"},{"sourceId":189656082,"sourceType":"kernelVersion"}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q libauc","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:04:19.466171Z","iopub.execute_input":"2024-08-06T00:04:19.466639Z","iopub.status.idle":"2024-08-06T00:04:34.520792Z","shell.execute_reply.started":"2024-08-06T00:04:19.466585Z","shell.execute_reply":"2024-08-06T00:04:34.519475Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import json\nimport time\nfrom pathlib import Path\nfrom io import BytesIO\n\nimport albumentations as A\nimport h5py\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom accelerate import Accelerator\nfrom accelerate.utils import (\n    DistributedDataParallelKwargs,\n    ProjectConfiguration,\n    set_seed,\n)\nfrom albumentations.pytorch import ToTensorV2\nfrom PIL import Image\nfrom sklearn.metrics import auc, roc_curve, roc_auc_score\nfrom timm import create_model\nfrom torch.utils.data import DataLoader, Dataset\nfrom libauc.sampler import DualSampler\nfrom libauc.losses import pAUCLoss\nfrom libauc.optimizers import SOPAs\n\nfrom isic_helper import DotDict, get_folds","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-06T00:17:26.264984Z","iopub.execute_input":"2024-08-06T00:17:26.265700Z","iopub.status.idle":"2024-08-06T00:17:31.820238Z","shell.execute_reply.started":"2024-08-06T00:17:26.265655Z","shell.execute_reply":"2024-08-06T00:17:31.819313Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def dev_augment(image_size):\n    transform = A.Compose(\n        [\n            A.Transpose(p=0.5),\n            A.VerticalFlip(p=0.5),\n            A.HorizontalFlip(p=0.5),\n            A.RandomBrightnessContrast(\n                brightness_limit=0.2, contrast_limit=0.2, p=0.75\n            ),\n            A.OneOf(\n                [\n                    A.MotionBlur(blur_limit=(5, 7)),\n                    A.MedianBlur(blur_limit=(5, 7)),\n                    A.GaussianBlur(blur_limit=(5, 7)),\n                    A.GaussNoise(var_limit=(5.0, 30.0)),\n                ],\n                p=0.7,\n            ),\n            A.OneOf(\n                [\n                    A.OpticalDistortion(distort_limit=1.0),\n                    A.GridDistortion(num_steps=5, distort_limit=1.0),\n                    A.ElasticTransform(alpha=3),\n                ],\n                p=0.7,\n            ),\n            A.CLAHE(clip_limit=4.0, p=0.7),\n            A.HueSaturationValue(\n                hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5\n            ),\n            A.ShiftScaleRotate(\n                shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.85\n            ),\n            A.Resize(image_size, image_size),\n            A.CoarseDropout(\n                max_height=int(image_size * 0.375),\n                max_width=int(image_size * 0.375),\n                max_holes=1,\n                min_holes=1,\n                p=0.7,\n            ),\n            A.Normalize(),\n            ToTensorV2(),\n        ],\n        p=1.0,\n    )\n    return transform\n\n\ndef val_augment(image_size):\n    transform = A.Compose(\n        [A.Resize(image_size, image_size), A.Normalize(), ToTensorV2()], p=1.0\n    )\n    return transform\n\n\nclass ISICDataset(Dataset):\n    def __init__(self, metadata, images, augment, infer=False):\n        self.metadata = metadata\n        self.images = images\n        self.targets = metadata[\"target\"].values\n        self.augment = augment\n        self.length = len(self.metadata)\n        self.infer = infer\n        if not infer:\n            self.value_counts_dict = self.metadata.target.value_counts().to_dict()\n            self.imratio = self.value_counts_dict[1]/self.value_counts_dict[0]\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, index):\n        row = self.metadata.iloc[index]\n\n        image = np.array(Image.open(BytesIO(self.images[row[\"isic_id\"]][()])))\n        image = self.augment(image=image)[\"image\"]\n\n        data = image.float()\n\n        if not self.infer:\n            target = torch.tensor(row[\"target\"]).long()\n            return data, target, index\n\n        return data, index\n\n\nclass ISICNet(nn.Module):\n    def __init__(self, model_name, pretrained=False):\n        super(ISICNet, self).__init__()\n        self.model = create_model(\n            model_name=model_name,\n            pretrained=pretrained,\n            in_chans=3,\n            num_classes=0,\n            global_pool=\"\",\n        )\n        self.classifier = nn.Linear(self.model.num_features, 1)\n        self.dropouts = nn.ModuleList([nn.Dropout(0.5) for _ in range(5)])\n\n    def forward(self, image):\n        x = self.model(image)\n        bs = len(image)\n        pool = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)\n\n        if self.training:\n            logit = 0\n            for i in range(len(self.dropouts)):\n                logit += self.classifier(self.dropouts[i](pool))\n            logit = logit / len(self.dropouts)\n        else:\n            logit = self.classifier(pool)\n        return logit\n\n    \ndef train_epoch(\n    epoch,\n    model,\n    optimizer,\n    criterion,\n    dev_dataloader,\n    accelerator,\n    log_interval=10,\n):\n    model.train()\n    train_loss = []\n    total_steps = len(dev_dataloader)\n    for step, (data, target, index) in enumerate(dev_dataloader):\n        optimizer.zero_grad()\n        logits = model(data)\n        probs = torch.sigmoid(logits)\n        loss = criterion(probs, target, index)\n        accelerator.backward(loss)\n        optimizer.step()\n\n        loss_value = accelerator.gather(loss).item()\n        train_loss.append(loss_value)\n        smooth_loss = sum(train_loss[-100:]) / min(len(train_loss), 100)\n        if step % log_interval == 0:\n            print(\n                f\"Epoch: {epoch} | Step: {step + 1}/{total_steps} |\"\n                f\" Loss: {loss_value:.5f} | Smooth loss: {smooth_loss:.5f}\"\n            )\n    train_loss = np.mean(train_loss)\n    return train_loss\n\n\ndef get_trans(img, iteration):\n    if iteration >= 6:\n        img = img.transpose(2, 3)\n    if iteration % 6 == 0:\n        return img\n    elif iteration % 6 == 1:\n        return torch.flip(img, dims=[2])\n    elif iteration % 6 == 2:\n        return torch.flip(img, dims=[3])\n    elif iteration % 6 == 3:\n        return torch.rot90(img, 1, dims=[2, 3])\n    elif iteration % 6 == 4:\n        return torch.rot90(img, 2, dims=[2, 3])\n    elif iteration % 6 == 5:\n        return torch.rot90(img, 3, dims=[2, 3])\n\n\ndef val_epoch(\n    epoch,\n    model,\n    criterion,\n    val_dataloader,\n    accelerator,\n    n_tta,\n    log_interval=50,\n):\n    model.eval()\n    val_probs = []\n    val_targets = []\n    val_loss = []\n    total_steps = len(val_dataloader)\n    with torch.no_grad():\n        for step, (data, target, index) in enumerate(val_dataloader):\n            logits = 0\n            probs = 0\n            for idx in range(n_tta):\n                logits_iter = model(get_trans(data, idx))\n                logits += logits_iter\n                probs += torch.sigmoid(logits_iter)\n            logits /= n_tta\n            probs /= n_tta\n\n            loss = criterion(probs, target, index)\n            val_loss.append(loss.detach().cpu().numpy())\n\n            logits, probs, targets = accelerator.gather((logits, probs, target))\n            val_probs.append(probs)\n            val_targets.append(targets)\n\n            if step % log_interval == 0:\n                print(f\"Epoch: {epoch} | Step: {step + 1}/{total_steps}\")\n\n    val_loss = np.mean(val_loss)\n    val_probs = torch.cat(val_probs).cpu().numpy()\n    val_targets = torch.cat(val_targets).cpu().numpy()\n    val_auc = compute_auc(val_targets, val_probs)\n    val_pauc = compute_pauc(val_targets, val_probs, min_tpr=0.8)\n    return (\n        val_loss,\n        val_auc,\n        val_pauc,\n        val_probs,\n        val_targets\n    )\n\ndef compute_auc(y_true, y_pred) -> float:\n    \"\"\"\n    Compute the Area Under the Receiver Operating Characteristic Curve (ROC AUC).\n\n    Args:\n        y_true: ground truth of 1s and 0s\n        y_pred: predictions of scores ranging [0, 1]\n\n    Returns:\n        Float value range [0, 1]\n    \"\"\"\n    return roc_auc_score(y_true, y_pred)\n\n\ndef compute_pauc(y_true, y_pred, min_tpr: float = 0.80) -> float:\n    \"\"\"\n    2024 ISIC Challenge metric: pAUC\n\n    Given a solution file and submission file, this function returns the\n    partial area under the receiver operating characteristic (pAUC)\n    above a given true positive rate (TPR) = 0.80.\n    https://en.wikipedia.org/wiki/Partial_Area_Under_the_ROC_Curve.\n\n    (c) 2024 Nicholas R Kurtansky, MSKCC\n\n    Args:\n        min_tpr:\n        y_true: ground truth of 1s and 0s\n        y_pred: predictions of scores ranging [0, 1]\n\n    Returns:\n        Float value range [0, max_fpr]\n    \"\"\"\n\n    # rescale the target. set 0s to 1s and 1s to 0s (since sklearn only has max_fpr)\n    v_gt = abs(y_true - 1)\n\n    # flip the submissions to their compliments\n    v_pred = -1.0 * y_pred\n\n    max_fpr = abs(1 - min_tpr)\n\n    # using sklearn.metric functions: (1) roc_curve and (2) auc\n    fpr, tpr, _ = roc_curve(v_gt, v_pred, sample_weight=None)\n    if max_fpr is None or max_fpr == 1:\n        return auc(fpr, tpr)\n    if max_fpr <= 0 or max_fpr > 1:\n        raise ValueError(\"Expected min_tpr in range [0, 1), got: %r\" % min_tpr)\n\n    # Add a single point at max_fpr by linear interpolation\n    stop = np.searchsorted(fpr, max_fpr, \"right\")\n    x_interp = [fpr[stop - 1], fpr[stop]]\n    y_interp = [tpr[stop - 1], tpr[stop]]\n    tpr = np.append(tpr[:stop], np.interp(max_fpr, x_interp, y_interp))\n    fpr = np.append(fpr[:stop], max_fpr)\n    partial_auc = auc(fpr, tpr)\n\n    #     # Equivalent code that uses sklearn's roc_auc_score\n    #     v_gt = abs(np.asarray(solution.values)-1)\n    #     v_pred = np.array([1.0 - x for x in submission.values])\n    #     max_fpr = abs(1-min_tpr)\n    #     partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n    #     # change scale from [0.5, 1.0] to [0.5 * max_fpr**2, max_fpr]\n    #     # https://math.stackexchange.com/questions/914823/shift-numbers-into-a-different-range\n    #     partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n\n    return partial_auc\n","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:17:31.822771Z","iopub.execute_input":"2024-08-06T00:17:31.823178Z","iopub.status.idle":"2024-08-06T00:17:31.868115Z","shell.execute_reply.started":"2024-08-06T00:17:31.823141Z","shell.execute_reply":"2024-08-06T00:17:31.866905Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"INPUT_DIR = Path(\"/kaggle/input\")\nARTIFACTS_DIR = Path(\".\")\n\nargs = DotDict()\n\nargs.data_dir = INPUT_DIR / \"isic-2024-challenge\"\nargs.model_name = \"efficientnet_b1\"\nargs.version = \"v1\"\nargs.model_identifier = f\"deep_auc_{args.model_name}_{args.version}\"\nargs.model_dir = ARTIFACTS_DIR / args.model_identifier\nargs.model_dir.mkdir(parents=True, exist_ok=True)\nargs.pretrained_model_path = INPUT_DIR / f\"isic-scd-{args.model_name.replace('_', '-')}-{args.version}-train\"\nargs.logging_dir = \"logs\"\nargs.fold = 1\n\nargs.mixed_precision = \"fp16\"\nargs.image_size = 128\nargs.batch_size = 64\nargs.num_workers = 4\nargs.init_lr = 1e-3\nargs.gamma = 500\nargs.margin = 1.0\nargs.weight_decay = 1e-5\nargs.num_epochs = 10\nargs.n_tta: int = 6\nargs.seed = 2022\nargs.debug = True","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:17:31.869398Z","iopub.execute_input":"2024-08-06T00:17:31.869770Z","iopub.status.idle":"2024-08-06T00:17:31.885531Z","shell.execute_reply.started":"2024-08-06T00:17:31.869739Z","shell.execute_reply":"2024-08-06T00:17:31.884471Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"logging_dir = Path(args.model_dir, args.logging_dir)\naccelerator_project_config = ProjectConfiguration(\n    project_dir=args.model_dir, logging_dir=str(logging_dir)\n)\nkwargs = DistributedDataParallelKwargs()\naccelerator = Accelerator(\n    mixed_precision=args.mixed_precision,\n    project_config=accelerator_project_config,\n    kwargs_handlers=[kwargs],\n)\nprint(accelerator.state)\n\nif args.seed is not None:\n    set_seed(args.seed)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:17:31.886772Z","iopub.execute_input":"2024-08-06T00:17:31.887078Z","iopub.status.idle":"2024-08-06T00:17:31.970687Z","shell.execute_reply.started":"2024-08-06T00:17:31.887051Z","shell.execute_reply":"2024-08-06T00:17:31.969668Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Distributed environment: NO\nNum processes: 1\nProcess index: 0\nLocal process index: 0\nDevice: cuda\n\nMixed precision type: fp16\n\n","output_type":"stream"}]},{"cell_type":"code","source":"train_metadata = pd.read_csv(args.data_dir / \"train-metadata.csv\", low_memory=False)\ntest_metadata = pd.read_csv(args.data_dir / \"test-metadata.csv\")\n\nfolds_df = get_folds()\ntrain_metadata = train_metadata.merge(folds_df, on=[\"isic_id\", \"patient_id\"], how=\"inner\")\nprint(f\"Train data size: {train_metadata.shape}\")\nprint(f\"Test data size: {test_metadata.shape}\")\n\ntrain_images = h5py.File(args.data_dir / \"train-image.hdf5\", mode=\"r\")\ntest_images = h5py.File(args.data_dir / \"test-image.hdf5\", mode=\"r\")","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:17:31.973059Z","iopub.execute_input":"2024-08-06T00:17:31.973377Z","iopub.status.idle":"2024-08-06T00:17:40.603222Z","shell.execute_reply.started":"2024-08-06T00:17:31.973349Z","shell.execute_reply":"2024-08-06T00:17:40.602204Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Train data size: (401059, 57)\nTest data size: (3, 44)\n","output_type":"stream"}]},{"cell_type":"code","source":"if args.debug:\n    train_metadata = train_metadata.sample(frac=0.05, random_state=args.seed)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:17:40.604608Z","iopub.execute_input":"2024-08-06T00:17:40.604940Z","iopub.status.idle":"2024-08-06T00:17:40.714633Z","shell.execute_reply.started":"2024-08-06T00:17:40.604912Z","shell.execute_reply":"2024-08-06T00:17:40.713658Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"dev_index = train_metadata[train_metadata[\"fold\"] != args.fold].index\nval_index = train_metadata[train_metadata[\"fold\"] == args.fold].index\n\ndev_metadata = train_metadata.loc[dev_index, :].reset_index(drop=True)\nval_metadata = train_metadata.loc[val_index, :].reset_index(drop=True)\n\ndev_dataset = ISICDataset(\n    dev_metadata, train_images, augment=dev_augment(args.image_size)\n)\nval_dataset = ISICDataset(\n    val_metadata, train_images, augment=val_augment(args.image_size)\n)\n\nsampling_rate = 0.1\nsampler = DualSampler(dev_dataset, args.batch_size, sampling_rate=sampling_rate)\n\ndev_dataloader = DataLoader(\n    dev_dataset,\n    batch_size=args.batch_size,\n    sampler=sampler,\n    num_workers=args.num_workers,\n)\nval_dataloader = DataLoader(\n    val_dataset,\n    batch_size=args.batch_size,\n    shuffle=False,\n    num_workers=args.num_workers,\n    drop_last=False,\n    pin_memory=True,\n)\n\nmodel = ISICNet(\n    model_name=args.model_name, pretrained=False,\n)\nmodel = model.to(accelerator.device)\n\ncriterion = pAUCLoss(\"1w\", data_len=len(dev_dataset), margin=args.margin, gamma=args.gamma)\noptimizer = SOPAs(model.parameters(), \n                 mode='adam', \n                 lr=args.init_lr, \n                 weight_decay=args.weight_decay)\n\n(\n    model,\n    optimizer,\n    dev_dataloader,\n    val_dataloader,\n) = accelerator.prepare(\n    model, optimizer, dev_dataloader, val_dataloader\n)\n\nmodel_filepath = args.pretrained_model_path / f\"models/fold_{args.fold}\"\naccelerator.load_state(model_filepath)\n\nbest_val_auc = 0\nbest_val_pauc = 0\nbest_val_loss = 0\nbest_epoch = 0\nbest_val_probs = None\n\nfor epoch in range(1, args.num_epochs + 1):\n    print(f\"Fold {args.fold} | Epoch {epoch}\")\n    start_time = time.time()\n\n    train_loss = train_epoch(\n        epoch,\n        model,\n        optimizer,\n        criterion,\n        dev_dataloader,\n        accelerator,\n    )\n    (\n        val_loss,\n        val_auc,\n        val_pauc,\n        val_probs,\n        val_targets,\n    ) = val_epoch(\n        epoch,\n        model,\n        criterion,\n        val_dataloader,\n        accelerator,\n        args.n_tta,\n    )\n\n    if val_pauc > best_val_pauc:\n        print(\n            f\"pAUC: {best_val_pauc:.5f} --> {val_pauc:.5f}, saving model...\"\n        )\n        best_val_pauc = val_pauc\n        best_val_auc = val_auc\n        best_val_loss = val_loss\n        best_epoch = epoch\n        best_val_probs = binary_probs\n        output_dir = f\"{args.model_dir}/models/fold_{args.fold}\"\n        accelerator.save_state(output_dir)\n    else:\n        print(\n            f\"pAUC: {best_val_pauc:.5f} --> {val_pauc:.5f}, skipping model save...\"\n        )\n    print(\n        f\"Fold: {args.fold} | Epoch: {epoch} |\"\n        f\" Train loss: {train_loss:.5f} | Val loss: {val_loss:.5f}\"\n        f\" Val AUC: {val_auc:.5f} | Val pAUC: {val_pauc:.5f}\"\n    )\n    elapsed_time = time.time() - start_time\n    elapsed_mins = int(elapsed_time // 60)\n    elapsed_secs = int(elapsed_time % 60)\n    print(f\"Epoch {epoch} took {elapsed_mins}m {elapsed_secs}s\")\n\nprint(\n    f\"Fold: {args.fold} | \"\n    f\"Best Val pAUC: {best_val_pauc} | Best AUC: {best_val_auc} |\"\n    f\" Best loss: {best_val_loss} |\"\n    f\" Best epoch: {best_epoch}\"\n)\noof_df = pd.DataFrame(\n    {\n        \"isic_id\": val_metadata[\"isic_id\"],\n        \"patient_id\": val_metadata[\"patient_id\"],\n        \"fold\": args.fold,\n        \"target\": val_metadata[\"target\"],\n        f\"oof_{args.model_identifier}\": best_val_probs,\n    }\n)\noof_df.to_csv(\n    f\"{args.model_dir}/oof_preds_{args.model_identifier}_fold_{args.fold}.csv\",\n    index=False,\n)\n\nfold_metadata = {\n    \"fold\": args.fold,\n    \"best_epoch\": best_epoch,\n    \"best_val_auc\": best_val_auc,\n    \"best_val_pauc\": best_val_pauc,\n    \"best_val_loss\": float(best_val_loss),\n}\nwith open(f\"{args.model_dir}/models/fold_{args.fold}/metadata.json\", \"w\") as f:\n    json.dump(fold_metadata, f)\nprint(f\"Finished training fold {args.fold}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:17:40.716111Z","iopub.execute_input":"2024-08-06T00:17:40.716515Z","iopub.status.idle":"2024-08-06T00:17:42.428581Z","shell.execute_reply.started":"2024-08-06T00:17:40.716478Z","shell.execute_reply":"2024-08-06T00:17:42.427071Z"},"trusted":true},"execution_count":7,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 53\u001b[0m\n\u001b[1;32m     43\u001b[0m (\n\u001b[1;32m     44\u001b[0m     model,\n\u001b[1;32m     45\u001b[0m     optimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m     model, optimizer, dev_dataloader, val_dataloader\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     52\u001b[0m model_filepath \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mpretrained_model_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/fold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 53\u001b[0m \u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_filepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m best_val_auc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     56\u001b[0m best_val_pauc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:3102\u001b[0m, in \u001b[0;36mAccelerator.load_state\u001b[0;34m(self, input_dir, **load_model_func_kwargs)\u001b[0m\n\u001b[1;32m   3099\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3100\u001b[0m         map_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 3102\u001b[0m \u001b[43mload_accelerator_state\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3103\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3105\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschedulers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3108\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3109\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3111\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mload_model_func_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3112\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3113\u001b[0m custom_checkpoints \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   3114\u001b[0m     f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(input_dir) \u001b[38;5;28;01mif\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^custom_checkpoint_\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.pkl$\u001b[39m\u001b[38;5;124m\"\u001b[39m, f) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3115\u001b[0m ]\n\u001b[1;32m   3116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(custom_checkpoints) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_custom_objects):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/checkpointing.py:204\u001b[0m, in \u001b[0;36mload_accelerator_state\u001b[0;34m(input_dir, models, optimizers, schedulers, dataloaders, process_index, scaler, map_location, **load_model_func_kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m         input_model_file \u001b[38;5;241m=\u001b[39m input_dir\u001b[38;5;241m.\u001b[39mjoinpath(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mending\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    203\u001b[0m         state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(input_model_file, map_location\u001b[38;5;241m=\u001b[39mmap_location)\n\u001b[0;32m--> 204\u001b[0m     \u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mload_model_func_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll model weights loaded successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# Optimizer states\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2148\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2149\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2153\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ISICNet:\n\tsize mismatch for classifier.weight: copying a param with shape torch.Size([9, 1280]) from checkpoint, the shape in current model is torch.Size([1, 1280]).\n\tsize mismatch for classifier.bias: copying a param with shape torch.Size([9]) from checkpoint, the shape in current model is torch.Size([1])."],"ename":"RuntimeError","evalue":"Error(s) in loading state_dict for ISICNet:\n\tsize mismatch for classifier.weight: copying a param with shape torch.Size([9, 1280]) from checkpoint, the shape in current model is torch.Size([1, 1280]).\n\tsize mismatch for classifier.bias: copying a param with shape torch.Size([9]) from checkpoint, the shape in current model is torch.Size([1]).","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}